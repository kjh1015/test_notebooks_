{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U --quiet langchain_openai langsmith langgraph langchain numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_openai import ChatOpenAI\n",
    "from math_tools import get_math_tool\n",
    "from forecast_tools import get_forecast_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate = get_math_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "# search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "class SearchSchema(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=\"The search query string to look up information. Example: 'current temperature in San Francisco'\"\n",
    "    )\n",
    "\n",
    "def search_func(query: Optional[str] = None, **kwargs: Any) -> str:\n",
    "    \"\"\"Execute a web search with the given query.\"\"\"\n",
    "    # Try to get query from various possible inputs\n",
    "    if not query:\n",
    "        if isinstance(kwargs.get('args'), dict):\n",
    "            query = kwargs['args'].get('query')\n",
    "            if not query and kwargs['args']:\n",
    "                # If no query field but has other values, use them\n",
    "                query = ' '.join(str(v) for v in kwargs['args'].values() if v)\n",
    "        elif isinstance(kwargs.get('args'), (list, tuple)):\n",
    "            query = ' '.join(str(x) for x in kwargs['args'] if x)\n",
    "        elif kwargs:\n",
    "            # Try to find any string value in kwargs\n",
    "            for val in kwargs.values():\n",
    "                if val:  # Check any non-empty value\n",
    "                    query = str(val)\n",
    "                    break\n",
    "    \n",
    "    if not query:\n",
    "        raise ValueError(\n",
    "            \"Search query cannot be empty. Please provide a query string. \"\n",
    "            \"Example: {'query': 'GDP of New York'} or {'args': {'query': 'GDP of New York'}}\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        search_runner = DuckDuckGoSearchRun()\n",
    "        result = search_runner.run(query)\n",
    "        if not result or result.lower().startswith('error'):\n",
    "            raise ValueError(f\"Search failed to return valid results for query: {query}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Search failed for query '{query}'. Error: {str(e)}\")\n",
    "\n",
    "search = StructuredTool(\n",
    "    name=\"search\",\n",
    "    func=search_func,\n",
    "    description=\"\"\"Search the internet for current information. \n",
    "    Required input: A search query string.\n",
    "    Example: search.invoke({\"query\": \"current GDP of New York\"})\n",
    "    Or: search.invoke({\"args\": {\"query\": \"current GDP of New York\"}})\n",
    "    \"\"\",\n",
    "    args_schema=SearchSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 2023, the real gross domestic product (GDP) of New York was about 1.78 trillion U.S. dollars. This is a slight increase from the previous year, when the state's GDP stood at 1.76 trillion U.S ... In 2023, GDP per person in New York was $91.5 thousand, up 2.0% from 2022. In 2023, real GDP was equivalent to $91,523 per person. Real gross domestic product per person in New York, chained 2017 dollars Learn how financial services, health care, professional and business services, retail trade, manufacturing, and educational services contribute to New York's $1.78 trillion GDP in 2023. Find out the number of workers, salaries, and products in each sector and how they rank nationally and globally. Find data on GDP by industry for New York state from 1997 to 2023. See the latest estimates, trends, and sources from the U.S. Bureau of Economic Analysis. Graph and download economic data for Total Gross Domestic Product for New York-Newark-Jersey City, NY-NJ-PA (MSA) (NGMP35620) from 2001 to 2023 about NJ, New York, PA, NY, industry, GDP, and USA.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of correct usage:\n",
    "search.invoke({\"query\": \"GDP of New York\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search, calculate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate.invoke(\n",
    "    {\n",
    "        \"problem\": \"What's the temp of sf + 5?\",\n",
    "        \"context\": [\"Thet empreature of sf is 32 degrees\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = get_forecast_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast.invoke({\n",
    "#     \"problem\": \"What's the temp of sf + 5? Given temperature series [32]\",\n",
    "#     \"context\": [\"The temperature of sf is 32 degrees\"],\n",
    "#     \"forecast\": \"temperature forecast\",\n",
    "#     \"Answer\": \"\",\n",
    "#     \"Forecast results with confidence intervals\": \"\",\n",
    "#     \"Question with forecasting problem.\": \"What's the temp of sf + 5?\",\n",
    "#     \"code\": \"\",\n",
    "#     \"forecasting expression that processes the data\": \"\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from output_parser import LLMCompilerPlanParser, Task\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='search' description='Search the internet for current information. \\n    Example input: {\"query\": \"current temperature in San Francisco\"}\\n    Example output: \"The current temperature in San Francisco is 65°F\" ' args_schema=<class '__main__.SearchSchema'> func=<function search_func at 0x00000282E5B296C0> {}\n",
      "---\n",
      "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'langchain_core.utils.pydantic.math'> func=<function get_math_tool.<locals>.calculate_expression at 0x00000282E5B28B80> {'context': ['The current temperature in San Francisco is 65°F']}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Fetching Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "    tool: BaseTool,\n",
    "    dependencies: number[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke, dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_messages = plan_and_schedule.invoke(\n",
    "    {\"messages\": [HumanMessage(content=example_question)]}\n",
    ")[\"messages\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content='Critical to Extremely Critical Fire Weather Conditions in Southern California; Heavy Lake Effect Snow Downwind of Lakes Erie and Ontario ... Current conditions at SAN FRANCISCO DOWNTOWN (SFOC1) Lat: 37.77056°NLon: 122.42694°WElev: 150.0ft. NA. 51°F. 11°C. Humidity: 43%: Wind Speed: NA NA MPH: Barometer: NA: Dewpoint: 29°F (-2°C ... San Francisco Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the San Francisco area. See the latest San Francisco weather forecast, current conditions, and live radar. Keep up to date on all San Francisco weather news with KRON4. Current Hazards. Daily Briefing; Submit Report; Current Outlooks; Detailed Hazards; Tsunami; Graphical Hazardous Weather Outlook; Current Conditions. Hydro Maps Data; ... National Weather Service San Francisco Bay Area, CA 21 Grace Hopper Ave, Stop 5 Monterey, CA 93943-5505 (831) 656-1725 Comments? Questions? Please Contact Us. ... Current Hazards. Daily Briefing; Submit Report; Current Outlooks; Detailed Hazards; Tsunami; Graphical Hazardous Weather Outlook; Current Conditions. Observations; ... National Weather Service San Francisco Bay Area, CA 21 Grace Hopper Ave, Stop 5 Monterey, CA 93943-5505 (831) 656-1725 Comments? Questions? Please Contact Us. ...', additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in San Francisco'}}, response_metadata={}, name='search', tool_call_id=1),\n",
       " FunctionMessage(content='14', additional_kwargs={'idx': 2, 'args': {'problem': 'pow($1, 3)', 'context': ['The current temperature in San Francisco is 65°F']}}, response_metadata={}, name='math', tool_call_id=2),\n",
       " FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', tool_call_id=3)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "runnable = joiner_prompt | llm.with_structured_output(\n",
    "    JoinOutputs, method=\"function_calling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=example_question)] + tool_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content=\"Thought: The temperature in San Francisco is reported as 51°F. To answer the user's question, this value needs to be cubed. However, the provided actions did not include the computation of 51°F raised to the 3rd power but instead showed a mathematical operation result unrelated to the user's query. Therefore, the missing computation needs to be performed to answer the question correctly.\", additional_kwargs={}, response_metadata={}),\n",
       "  SystemMessage(content=\"Context from last attempt: The mathematical computation needed to answer the question - cubing the current temperature in San Francisco - was not performed. The result provided does not match the user's request.\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joiner.invoke({\"messages\": input_messages})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='ERROR(Failed to call search with args {}. Args resolved to {}. Error: 1 validation error for SearchSchema\\nquery\\n  Field required [type=missing, input_value={}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.10/v/missing)', additional_kwargs={'idx': 1, 'args': {}}, response_metadata={}, name='search', id='f2bbffc7-af5c-4760-a3d0-18835c1417ce', tool_call_id=1)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The search action failed due to a missing query parameter, so no information about New York's GDP was retrieved.\", additional_kwargs={}, response_metadata={}, id='25cb9aee-7613-4395-a465-8d96fedb7da7'), SystemMessage(content=\"Context from last attempt: The plan to obtain New York's GDP was not executed successfully due to a missing query parameter in the search action. A query specifying the intent to find the GDP of New York needs to be provided for a successful execution.\", additional_kwargs={}, response_metadata={}, id='fea25086-10b8-4b4e-b3af-922d12d131a7')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: The failed search action prevents us from providing the GDP of New York. We need to correct the search parameters and execute a new search.', additional_kwargs={}, response_metadata={}, id='649d0622-f0de-4bf9-934b-4c17d22d68cb'), SystemMessage(content=\"Context from last attempt: A valid search query for New York's GDP was not provided. Correct the mistake by specifying a search query that targets information about the GDP of New York.\", additional_kwargs={}, response_metadata={}, id='8c913294-2464-4f3e-ace1-03e9c0b7d763')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='In 2023, the real gross domestic product (GDP) of New York was about 1.78 trillion U.S. ... BEA, Real gross domestic product of New York in the United States from 2000 to 2023 (in billion U.S ... In 2023, GDP per person in New York was $91.5 thousand, up 2.0% from 2022. In 2023, real GDP was equivalent to $91,523 per person. Real gross domestic product per person in New York, chained 2017 dollars Real gross domestic product increased in 46 states and the District of Columbia in the third quarter of 2024, with the percent change ranging from 6.9 percent at an annual rate in Arkansas to -2.3 percent in North Dakota. ... New and Updated Estimates of the Regional Economic Accounts: Results of the 2023 Comprehensive ... December 2023; The ... To put this unequal distribution in perspective, this map compares U.S. state economies by their gross domestic product (GDP) to that of a similarly-sized country. State GDP data is sourced from the Bureau of Economic Analysis , the latest available for the year 2023. Graph and download economic data for Gross Domestic Product: All Industry Total in New York (NYNGSP) from 1997 to 2023 about GSP, NY, industry, GDP, and USA.', additional_kwargs={'idx': 2, 'args': {'query': 'GDP of New York 2023'}}, response_metadata={}, name='search', id='ae946c88-d58c-4f8b-88e4-7209310ea501', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='46323f6e-7206-49a7-9d59-8c6bb337b980', tool_call_id=3)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The search successfully retrieved the information needed to answer the user's question about New York's GDP for 2023.\", additional_kwargs={}, response_metadata={}, id='f9df1259-b134-46cb-bf2f-3831276d557a'), AIMessage(content='In 2023, the real gross domestic product (GDP) of New York was about 1.78 trillion U.S. dollars.', additional_kwargs={}, response_metadata={}, id='8f102271-1011-430c-b5ed-0cce15e59dab')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2023, the real gross domestic product (GDP) of New York was about 1.78 trillion U.S. dollars.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-hop question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='ERROR(Failed to call search with args {}. Args resolved to {}. Error: 1 validation error for SearchSchema\\nquery\\n  Field required [type=missing, input_value={}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.10/v/missing)', additional_kwargs={'idx': 1, 'args': {}}, response_metadata={}, name='search', id='1331d2cb-9361-43eb-aa54-bad8b9ac0d97', tool_call_id=1), FunctionMessage(content='ERROR(Failed to call search with args {}. Args resolved to {}. Error: 1 validation error for SearchSchema\\nquery\\n  Field required [type=missing, input_value={}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.10/v/missing)', additional_kwargs={'idx': 2, 'args': {}}, response_metadata={}, name='search', id='2303e92a-c29e-47e0-bade-5faf7c09bbb6', tool_call_id=2), FunctionMessage(content=\"ERROR(Failed to call math with args {'context': ['oldest parrot alive age', 'average lifespan of parrots']}. Args resolved to {'context': ['oldest parrot alive age', 'average lifespan of parrots']}. Error: 1 validation error for math\\nproblem\\n  Field required [type=missing, input_value={'context': ['oldest parr...e lifespan of parrots']}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.10/v/missing)\", additional_kwargs={'idx': 3, 'args': {'context': ['oldest parrot alive age', 'average lifespan of parrots']}}, response_metadata={}, name='math', id='7610b38c-90bf-41a8-8cff-cc022e6e15cd', tool_call_id=3), FunctionMessage(content='join', additional_kwargs={'idx': 4, 'args': ()}, response_metadata={}, name='join', id='4be77eb8-74c1-492d-a690-4299bc0d77b6', tool_call_id=4)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: Since there was an error in executing search, no information has been retrieved to answer the user's question about the oldest parrot alive and how it compares to the average lifespan of parrots.\", additional_kwargs={}, response_metadata={}, id='ebd2f6ee-550c-417e-958f-e64ebb0e5b91'), SystemMessage(content='Context from last attempt: Need to successfully execute a search to find information about the oldest parrot currently alive and the average lifespan of parrots to compare.', additional_kwargs={}, response_metadata={}, id='389ae1c0-c568-4dad-9283-12d3752230ed')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: Since there was an error in executing search, no information has been retrieved to answer the user's question about the oldest parrot alive and how it compares to the average lifespan of parrots.\", additional_kwargs={}, response_metadata={}, id='f35f1940-aefb-495b-a16e-8dad4ab58c18'), SystemMessage(content='Context from last attempt: Need to successfully execute a search to find information about the oldest parrot currently alive and the average lifespan of parrots to compare.', additional_kwargs={}, response_metadata={}, id='f6512148-af1d-473b-8ed6-0a1363ba3807')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: Given that no searches were successfully executed to find the required information, and no relevant data has been provided, a conclusion cannot be drawn.', additional_kwargs={}, response_metadata={}, id='5348001c-5349-4e68-965d-b1d5ccf8f132'), SystemMessage(content=\"Context from last attempt: To answer the user's question, a successful search need to be executed to find information about the oldest parrot alive and how it compares to the average lifespan of parrots.\", additional_kwargs={}, response_metadata={}, id='61189409-bdb8-4bf7-86e6-0674136baf27')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: Given that no searches were successfully executed to find the required information, and no relevant data has been provided, a conclusion cannot be drawn.', additional_kwargs={}, response_metadata={}, id='3a503c87-f7b8-4386-a7df-7e27abe25342'), SystemMessage(content=\"Context from last attempt: The errors in executing search actions prevented the retrieval of necessary information to answer the user's question. A successful search needs to be conducted to find information about the oldest parrot alive and its life expectancy compared to the average lifespan of parrots.\", additional_kwargs={}, response_metadata={}, id='f1f76b34-a2b6-4143-91d7-16d92ac4a1e2')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: Given that no searches were successfully executed to find the required information, and no relevant data has been provided, a conclusion cannot be drawn.', additional_kwargs={}, response_metadata={}, id='bc927221-fa6a-4404-a426-c02d4e1e3f73'), SystemMessage(content='Context from last attempt: Need to execute a successful search to find current information about the oldest parrot alive and the average lifespan of parrots to compare them.', additional_kwargs={}, response_metadata={}, id='c706003e-b190-4086-a273-c247b6e3372a')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: Given that no searches were successfully executed to find the required information, and no relevant data has been provided, a conclusion cannot be drawn.', additional_kwargs={}, response_metadata={}, id='f28a0611-2fe3-4c85-aaff-e6b10f165be1'), SystemMessage(content=\"Context from last attempt: A search was not successfully executed to find information about the oldest parrot alive and how much longer that is than the average lifespan of parrots. Therefore, we're unable to provide an answer to the question at this time. It may be helpful to directly search for recent records of the oldest living parrot and compare that to general information about parrot lifespans which typically ranges based on species but can often be 20-50 years in captivity.\", additional_kwargs={}, response_metadata={}, id='90756a59-a1f3-4af9-bce4-3b8e0bed70d4')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: Given that no searches were successfully executed to find the required information, and no relevant data has been provided, a conclusion cannot be drawn.', additional_kwargs={}, response_metadata={}, id='5a1fc81d-82e2-4da7-84de-48fd5d517ed5'), SystemMessage(content=\"Context from last attempt: There was no information retrieved due to failed search attempts. It may be useful to directly search for the oldest living parrot's age and the average lifespan of parrots to answer this question.\", additional_kwargs={}, response_metadata={}, id='779da85e-893d-4125-ba85-0904b4738f72')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: Given the repeated failures to execute a search and without any successful information retrieval, it's clear we cannot provide the specific answer at this time.\", additional_kwargs={}, response_metadata={}, id='73b5ff82-b0d0-4b1e-a6c4-d75377290349'), AIMessage(content=\"I'm unable to find the current information on the oldest parrot alive and how much longer that is than the average lifespan of parrots due to search execution errors. For the most accurate and recent information, I recommend checking reputable sources or databases that track animal records directly.\", additional_kwargs={}, response_metadata={}, id='c1d29e20-ac14-4a64-9e39-b6d8431e10c3')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "steps = chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"recursion_limit\": 100,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to find the current information on the oldest parrot alive and how much longer that is than the average lifespan of parrots due to search execution errors. For the most accurate and recent information, I recommend checking reputable sources or databases that track animal records directly.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-step math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='3307.0', additional_kwargs={'idx': 1, 'args': {'problem': '((3*(4+5)/0.5)+3245) + 8'}}, response_metadata={}, name='math', id='4ef65a07-db80-4097-a7b6-fdd82da3337f', tool_call_id=1), FunctionMessage(content='7.565011820330969', additional_kwargs={'idx': 2, 'args': {'problem': '32/4.23'}}, response_metadata={}, name='math', id='8902cb4f-5ab8-40d2-9364-d31347542c28', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='92c23ba5-bacb-4c3d-952c-f71d675bf4a8', tool_call_id=3)]}}\n",
      "{'join': {'messages': [AIMessage(content='Thought: The computation results for the two separate expressions provided by the user have been obtained: ((3*(4+5)/0.5)+3245) + 8 = 3307.0 and 32/4.23 = 7.565011820330969. To answer the final query, the sum of these two values is needed.', additional_kwargs={}, response_metadata={}, id='534f5f04-4fec-4143-98ca-1fc2190fb085'), AIMessage(content='The answer to the first calculation is 3307.0, the answer to the second calculation is approximately 7.57 (rounded to two decimal places), and the sum of those two values is approximately 3314.57.', additional_kwargs={}, response_metadata={}, id='179aa802-ae3c-47de-8c43-444902df44f8')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's ((3*(4+5)/0.5)+3245) + 8? What's 32/4.23? What's the sum of those two values?\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the first calculation is 3307.0, the answer to the second calculation is approximately 7.57 (rounded to two decimal places), and the sum of those two values is approximately 3314.57.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Replanning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"In Tokyo, currently, the weather continues to be fair. The temperature is a frosty 9°C (48.2°F). The current temperature is still relatively far from the lowest expected temperature of 3°C (37.4°F). The temperature in Tokyo today in the early morning is 7 ° C. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 5 ° C. The chance of rain in Tokyo in the morning is 0%, and the wind will blow at 10 km/h. Current Tokyo weather condition is Clear with real-time temperature (8°C), humidity 54%, wind 10.1km/h, pressure (1016mb), UV (0), visibility (10km) in Tokyo. ... The monthly weather averages in Tokyo consist of 12 sunny days, 8 cloudy days, 9 rainy days, and 0 snowy days. Tokyo - Weather Conditions. Japan. Locations. Temp. Condition. Today's and tonight's professional weather forecast for Tokyo. Precipitation radar, HD satellite images, and current weather warnings, hourly temperature, chance of rain, and sunshine hours. Tokyo 7 day weather forecast including weather warnings, temperature, rain, wind, visibility, humidity and UV\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in Tokyo'}}, response_metadata={}, name='search', id='615146fa-980d-4218-8db1-7f5e3822da01', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='4d24cb4b-723f-4ea5-b285-fd16e4eb6bbd', tool_call_id=2)]}}\n",
      "{'join': {'messages': [AIMessage(content='Thought: The relevant information needed to create a flashcard summarizing the current temperature in Tokyo has been gathered.', additional_kwargs={}, response_metadata={}, id='657fe944-4890-4e4c-9b67-9626bbe9feab'), AIMessage(content='**Flashcard: Current Temperature in Tokyo**\\n\\n- **Temperature:** 8°C (46.4°F)\\n- **Feels Like:** 5°C (due to wind, humidity, and other conditions)\\n- **Condition:** Clear\\n- **Humidity:** 54%\\n- **Wind:** 10.1 km/h\\n- **Pressure:** 1016 mb\\n- **Visibility:** 10 km\\n- **Current Weather Assessment:** Fair, with a chance of rain at 0%.', additional_kwargs={}, response_metadata={}, id='e7c11974-e976-43c5-9ab5-fc060b1dd26c')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Find the current temperature in Tokyo, then, respond with a flashcard summarizing this information\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
