{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://langchain-ai.github.io/langgraph/tutorials/tnt-llm/tnt-llm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TNT-LLM has three main phases:\n",
    "\n",
    "1. Generate Taxonomy\n",
    "2. Label Training Data\n",
    "3. Finetune classifier + deploy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the taxonomy, TNT-LLM proposes 5 steps:\n",
    "\n",
    "1. **Summarize** chat logs using a lower-cost LLM (batched over all logs in the sample)\n",
    "2. **Batch** the logs into random minibatches\n",
    "3. **Generate** an initial taxonomy from the first minibatch\n",
    "4. **Update** the taxonomy on each subsequent minibatch via a ritique and revise prompt\n",
    "5. **Review** the final taxonomy, scoring its quality and generating a final value using a final sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain_anthropic langsmith langchain-community\n",
    "%pip install -U sklearn langchain_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import operator\n",
    "from typing import Annotated, List, Optional\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(\"tnt-llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc(TypedDict):\n",
    "    id: str\n",
    "    content: str\n",
    "    summary: Optional[str]\n",
    "    explanation: Optional[str]\n",
    "    category: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomyGenerationState(TypedDict):\n",
    "    # The raw docs; we inject summaries within them in the first step\n",
    "    documents: List[Doc]\n",
    "    # Indices to be concise\n",
    "    minibatches: List[List[int]]\n",
    "    # Candidate Taxonomies (full trajectory)\n",
    "    clusters: Annotated[List[List[dict]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Summarize Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "summary_prompt = hub.pull(\"wfh/tnt-llm-summary-generation\").partial(\n",
    "    summary_length=20, explanation_length=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['content'], input_types={}, partial_variables={'summary_length': 20, 'explanation_length': 30}, metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'tnt-llm-summary-generation', 'lc_hub_commit_hash': 'fc880e038931afd0ff84bf0191ee75be19aa25a6595bd4d7675f8c12b23685a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['content'], input_types={}, partial_variables={}, template='# Instruction\\n\\n## Context\\n- **Goal**: You are tasked with summarizing the input text for the given use case. The summary will represent the input data for clustering in the next step.\\n- **Data**: Your input data is a conversation history between a User and an AI agent.\\n\\n# Data\\n<data>\\n{content}\\n</data>\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['explanation_length', 'summary_length'], input_types={}, partial_variables={}, template='# Questions\\n## Q1. Summarize the input text in {summary_length} words or less for the use case.\\nWrite the summary between <summary> </summary> tags.\\n\\nTips:\\n- The summary should contain the relevant information for the use case in as much detail as possible.\\n- Be concise and clear. Do not add phrases like \"This is the summary of the data ...\" or \"Summarized text: ...\".\\n- Similarly, do not reference the user (\\'the user asked XYZ\\') unless it\\'s absolutely relevant.\\n- Within {summary_length} words, include as much relevant information as possible.\\n- Do not include any line breaks in the summary.\\n- Provide your answer in **English** only.\\n\\n## Q2. Explain how you wrote the summary in {explanation_length} words or less.\\n\\n## Provide your answers between the tags <summary>your answer to Q1</summary>, <explanation>your answer to Q2</explanation>\\n\\n# Output\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary(xml_string: str) -> dict:\n",
    "    summary_pattern = r\"<summary>(.*?)</summary>\"\n",
    "    explanation_pattern = r\"<explanation>(.*?)</explanation>\"\n",
    "\n",
    "    summary_match = re.search(summary_pattern, xml_string, re.DOTALL)\n",
    "    explanation_match = re.search(explanation_pattern, xml_string, re.DOTALL)\n",
    "\n",
    "    summary = summary_match.group(1).strip() if summary_match else \"\"\n",
    "    explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
    "\n",
    "    return {\"summary\": summary, \"explanation\": explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_llm_chain = (\n",
    "    summary_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\") | StrOutputParser()\n",
    "    # Customize the tracing name for easier organization\n",
    ").with_config(run_name=\"GenerateSummary\")\n",
    "summary_chain = summary_llm_chain | parse_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine as a \"map\" operation in a map-reduce chain\n",
    "# Input: state\n",
    "# Output: state U summaries\n",
    "# Processes docs in parallel\n",
    "def get_content(state: TaxonomyGenerationState):\n",
    "    docs = state[\"documents\"]\n",
    "    return [{\"content\": doc[\"content\"]} for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_step = RunnablePassthrough.assign(\n",
    "    summaries=get_content\n",
    "    # This effectively creates a \"map\" operation\n",
    "    # Note you can make this more robust by handling individual errors\n",
    "    | RunnableLambda(func=summary_chain.batch, afunc=summary_chain.abatch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_summaries(combined: dict) -> TaxonomyGenerationState:\n",
    "    summaries = combined[\"summaries\"]\n",
    "    documents = combined[\"documents\"]\n",
    "    return {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"id\": doc[\"id\"],\n",
    "                \"content\": doc[\"content\"],\n",
    "                \"summary\": summ_info[\"summary\"],\n",
    "                \"explanation\": summ_info[\"explanation\"],\n",
    "            }\n",
    "            for doc, summ_info in zip(documents, summaries)\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is actually the node itself!\n",
    "map_reduce_chain = map_step | reduce_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split into Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_minibatches(state: TaxonomyGenerationState, config: RunnableConfig):\n",
    "    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n",
    "    original = state[\"documents\"]\n",
    "    indices = list(range(len(original)))\n",
    "    random.shuffle(indices)\n",
    "    if len(indices) < batch_size:\n",
    "        # Don't pad needlessly if we can't fill a single batch\n",
    "        return [indices]\n",
    "\n",
    "    num_full_batches = len(indices) // batch_size\n",
    "\n",
    "    batches = [\n",
    "        indices[i * batch_size : (i + 1) * batch_size] for i in range(num_full_batches)\n",
    "    ]\n",
    "\n",
    "    leftovers = len(indices) % batch_size\n",
    "    if leftovers:\n",
    "        last_batch = indices[num_full_batches * batch_size :]\n",
    "        elements_to_add = batch_size - leftovers\n",
    "        last_batch += random.sample(indices, elements_to_add)\n",
    "        batches.append(last_batch)\n",
    "\n",
    "    return {\n",
    "        \"minibatches\": batches,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Taxonomy Generation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "\n",
    "def parse_taxa(output_text: str) -> Dict:\n",
    "    \"\"\"Extract the taxonomy from the generated output.\"\"\"\n",
    "    cluster_matches = re.findall(\n",
    "        r\"\\s*<id>(.*?)</id>\\s*<name>(.*?)</name>\\s*<description>(.*?)</description>\\s*\",\n",
    "        output_text,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "    clusters = [\n",
    "        {\"id\": id.strip(), \"name\": name.strip(), \"description\": description.strip()}\n",
    "        for id, name, description in cluster_matches\n",
    "    ]\n",
    "    # We don't parse the explanation since it isn't used downstream\n",
    "    return {\"clusters\": clusters}\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Doc]) -> str:\n",
    "    xml_table = \"<conversations>\\n\"\n",
    "    for doc in docs:\n",
    "        xml_table += f'<conv_summ id={doc[\"id\"]}>{doc[\"summary\"]}</conv_summ>\\n'\n",
    "    xml_table += \"</conversations>\"\n",
    "    return xml_table\n",
    "\n",
    "\n",
    "def format_taxonomy(clusters):\n",
    "    xml = \"<cluster_table>\\n\"\n",
    "    for label in clusters:\n",
    "        xml += \"  <cluster>\\n\"\n",
    "        xml += f'    <id>{label[\"id\"]}</id>\\n'\n",
    "        xml += f'    <name>{label[\"name\"]}</name>\\n'\n",
    "        xml += f'    <description>{label[\"description\"]}</description>\\n'\n",
    "        xml += \"  </cluster>\\n\"\n",
    "    xml += \"</cluster_table>\"\n",
    "    return xml\n",
    "\n",
    "\n",
    "def invoke_taxonomy_chain(\n",
    "    chain: Runnable,\n",
    "    state: TaxonomyGenerationState,\n",
    "    config: RunnableConfig,\n",
    "    mb_indices: List[int],\n",
    ") -> TaxonomyGenerationState:\n",
    "    configurable = config[\"configurable\"]\n",
    "    docs = state[\"documents\"]\n",
    "    minibatch = [docs[idx] for idx in mb_indices]\n",
    "    data_table_xml = format_docs(minibatch)\n",
    "\n",
    "    previous_taxonomy = state[\"clusters\"][-1] if state[\"clusters\"] else []\n",
    "    cluster_table_xml = format_taxonomy(previous_taxonomy)\n",
    "\n",
    "    updated_taxonomy = chain.invoke(\n",
    "        {\n",
    "            \"data_xml\": data_table_xml,\n",
    "            \"use_case\": configurable[\"use_case\"],\n",
    "            \"cluster_table_xml\": cluster_table_xml,\n",
    "            \"suggestion_length\": configurable.get(\"suggestion_length\", 30),\n",
    "            \"cluster_name_length\": configurable.get(\"cluster_name_length\", 10),\n",
    "            \"cluster_description_length\": configurable.get(\n",
    "                \"cluster_description_length\", 30\n",
    "            ),\n",
    "            \"explanation_length\": configurable.get(\"explanation_length\", 20),\n",
    "            \"max_num_clusters\": configurable.get(\"max_num_clusters\", 25),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"clusters\": [updated_taxonomy[\"clusters\"]],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate initial taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# We will share an LLM for each step of the generate -> update -> review cycle\n",
    "# You may want to consider using Opus or another more powerful model for this\n",
    "taxonomy_generation_llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", max_tokens_to_sample=2000\n",
    ")\n",
    "\n",
    "\n",
    "## Initial generation\n",
    "taxonomy_generation_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-generation\").partial(\n",
    "    use_case=\"Generate the taxonomy that can be used to label the user intent in the conversation.\",\n",
    ")\n",
    "\n",
    "taxa_gen_llm_chain = (\n",
    "    taxonomy_generation_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name=\"GenerateTaxonomy\")\n",
    "\n",
    "\n",
    "generate_taxonomy_chain = taxa_gen_llm_chain | parse_taxa\n",
    "\n",
    "\n",
    "def generate_taxonomy(\n",
    "    state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    return invoke_taxonomy_chain(\n",
    "        generate_taxonomy_chain, state, config, state[\"minibatches\"][0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Update Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "taxonomy_update_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-update\")\n",
    "\n",
    "taxa_update_llm_chain = (\n",
    "    taxonomy_update_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name=\"UpdateTaxonomy\")\n",
    "\n",
    "\n",
    "update_taxonomy_chain = taxa_update_llm_chain | parse_taxa\n",
    "\n",
    "\n",
    "def update_taxonomy(\n",
    "    state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    which_mb = len(state[\"clusters\"]) % len(state[\"minibatches\"])\n",
    "    return invoke_taxonomy_chain(\n",
    "        update_taxonomy_chain, state, config, state[\"minibatches\"][which_mb]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Review Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "taxonomy_review_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-review\")\n",
    "\n",
    "taxa_review_llm_chain = (\n",
    "    taxonomy_review_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name=\"ReviewTaxonomy\")\n",
    "\n",
    "\n",
    "review_taxonomy_chain = taxa_review_llm_chain | parse_taxa\n",
    "\n",
    "\n",
    "def review_taxonomy(\n",
    "    state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n",
    "    original = state[\"documents\"]\n",
    "    indices = list(range(len(original)))\n",
    "    random.shuffle(indices)\n",
    "    return invoke_taxonomy_chain(\n",
    "        review_taxonomy_chain, state, config, indices[:batch_size]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(TaxonomyGenerationState)\n",
    "graph.add_node(\"summarize\", map_reduce_chain)\n",
    "graph.add_node(\"get_minibatches\", get_minibatches)\n",
    "graph.add_node(\"generate_taxonomy\", generate_taxonomy)\n",
    "graph.add_node(\"update_taxonomy\", update_taxonomy)\n",
    "graph.add_node(\"review_taxonomy\", review_taxonomy)\n",
    "\n",
    "graph.add_edge(\"summarize\", \"get_minibatches\")\n",
    "graph.add_edge(\"get_minibatches\", \"generate_taxonomy\")\n",
    "graph.add_edge(\"generate_taxonomy\", \"update_taxonomy\")\n",
    "\n",
    "\n",
    "def should_review(state: TaxonomyGenerationState) -> str:\n",
    "    num_minibatches = len(state[\"minibatches\"])\n",
    "    num_revisions = len(state[\"clusters\"])\n",
    "    if num_revisions < num_minibatches:\n",
    "        return \"update_taxonomy\"\n",
    "    return \"review_taxonomy\"\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"update_taxonomy\",\n",
    "    should_review,\n",
    "    # Optional (but required for the diagram to be drawn correctly below)\n",
    "    {\"update_taxonomy\": \"update_taxonomy\", \"review_taxonomy\": \"review_taxonomy\"},\n",
    ")\n",
    "graph.add_edge(\"review_taxonomy\", END)\n",
    "\n",
    "graph.add_edge(START, \"summarize\")\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    },
    {
     "ename": "LangSmithAuthError",
     "evalue": "Authentication failed for /sessions. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/sessions?limit=1&name=YOUR+PROJECT+NAME&include_stats=False', '{\"detail\":\"Invalid token\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\utils.py:150\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/sessions?limit=1&name=YOUR+PROJECT+NAME&include_stats=False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:783\u001b[0m, in \u001b[0;36mrequest_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithAPIError(\n\u001b[0;32m    778\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer error caused failure to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m LangSmith API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m     )\n\u001b[1;32m--> 783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m408\u001b[39m:\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithRequestTimeout(\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient took too long to send request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    787\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\utils.py:152\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHTTPError\u001b[0m: [Errno 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/sessions?limit=1&name=YOUR+PROJECT+NAME&include_stats=False] {\"detail\":\"Invalid token\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLangSmithAuthError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[0;32m      8\u001b[0m past_week \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_runs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meq(is_root, true)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_week\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# We only need to return the inputs + outputs\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert the langsmith traces to our graph's Doc object.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_to_doc\u001b[39m(run) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:1978\u001b[0m, in \u001b[0;36mlist_runs\u001b[1;34m(self, project_id, project_name, run_type, trace_id, reference_example_id, query, filter, trace_filter, tree_filter, is_root, parent_run_id, start_time, error, run_ids, select, limit, **kwargs)\u001b[0m\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1977\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hide_run_inputs(inputs)\n\u001b[1;32m-> 1978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_multipart:\n\u001b[0;32m   1980\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m ls_utils\u001b[38;5;241m.\u001b[39mdeepish_copy(outputs)\n",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\utils.py:138\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     )\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:2564\u001b[0m, in \u001b[0;36mread_project\u001b[1;34m(self, project_id, project_name, include_stats)\u001b[0m\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cursor_paginated_list(\n\u001b[0;32m   2558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/public/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_uuid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/runs/query\u001b[39m\u001b[38;5;124m\"\u001b[39m, body\u001b[38;5;241m=\u001b[39mbody\n\u001b[0;32m   2559\u001b[0m     ):\n\u001b[0;32m   2560\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mRun(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrun, _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_dataset_shared_schema\u001b[39m(\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m-> 2564\u001b[0m     dataset_id: Optional[ID_TYPE] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     dataset_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ls_schemas\u001b[38;5;241m.\u001b[39mDatasetShareSchema:\n\u001b[0;32m   2568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve the shared schema of a dataset.\u001b[39;00m\n\u001b[0;32m   2569\u001b[0m \n\u001b[0;32m   2570\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;124;03m        ValueError: If neither `dataset_id` nor `dataset_name` is given.\u001b[39;00m\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dataset_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:818\u001b[0m, in \u001b[0;36mrequest_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithUserError(\n\u001b[0;32m    815\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in LangSmith API.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         )\n\u001b[1;32m--> 818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mConnectionError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    819\u001b[0m     recommendation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease confirm your LANGCHAIN_ENDPOINT.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.smith.langchain.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease confirm your internet connection.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    823\u001b[0m     )\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mLangSmithAuthError\u001b[0m: Authentication failed for /sessions. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/sessions?limit=1&name=YOUR+PROJECT+NAME&include_stats=False', '{\"detail\":\"Invalid token\"}')"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "project_name = \"YOUR PROJECT NAME\"  # Update to your own project\n",
    "client = Client()\n",
    "\n",
    "past_week = datetime.now() - timedelta(days=7)\n",
    "runs = list(\n",
    "    client.list_runs(\n",
    "        project_name=project_name,\n",
    "        filter=\"eq(is_root, true)\",\n",
    "        start_time=past_week,\n",
    "        # We only need to return the inputs + outputs\n",
    "        select=[\"inputs\", \"outputs\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the langsmith traces to our graph's Doc object.\n",
    "def run_to_doc(run) -> Doc:\n",
    "    turns = []\n",
    "    idx = 0\n",
    "    for turn in run.inputs.get(\"chat_history\") or []:\n",
    "        key, value = next(iter(turn.items()))\n",
    "        turns.append(f\"<{key} idx={idx}>\\n{value}\\n</{key}>\")\n",
    "        idx += 1\n",
    "    turns.append(\n",
    "        f\"\"\"\n",
    "<human idx={idx}>\n",
    "{run.inputs['question']}\n",
    "</human>\"\"\"\n",
    "    )\n",
    "    if run.outputs and run.outputs[\"output\"]:\n",
    "        turns.append(\n",
    "            f\"\"\"<ai idx={idx+1}>\n",
    "{run.outputs['output']}\n",
    "</ai>\"\"\"\n",
    "        )\n",
    "    return {\n",
    "        \"id\": str(run.id),\n",
    "        \"content\": (\"\\n\".join(turns)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: An error occurred: Error code: 401 - {'error': {'message': 'The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.', 'type': 'invalid_request_error', 'param': None, 'code': 'account_deactivated'}}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Replace with your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def ask_openai(prompt):\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\" depending on your needs\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Extract the response text\n",
    "        answer = response.choices[0].message.content\n",
    "        return answer\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is the capital of France?\"\n",
    "    response = ask_openai(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response Object:\n",
      "Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))\n",
      "\n",
      "All Response Attributes:\n",
      "construct: <bound method BaseModel.construct of <class 'anthropic.types.message.Message'>>\n",
      "content: [TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')]\n",
      "copy: <bound method BaseModel.copy of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "dict: <bound method BaseModel.dict of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "from_orm: <bound method BaseModel.from_orm of <class 'anthropic.types.message.Message'>>\n",
      "id: msg_01JmHMthANCqW2tiLELDSecQ\n",
      "json: <bound method BaseModel.json of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "model: claude-3-opus-20240229\n",
      "model_computed_fields: {}\n",
      "model_config: {'extra': 'allow', 'defer_build': True}\n",
      "model_construct: <bound method BaseModel.construct of <class 'anthropic.types.message.Message'>>\n",
      "model_copy: <bound method BaseModel.model_copy of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "model_dump: <bound method BaseModel.model_dump of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "model_dump_json: <bound method BaseModel.model_dump_json of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "model_extra: {}\n",
      "model_fields: {'id': FieldInfo(annotation=str, required=True), 'content': FieldInfo(annotation=List[Annotated[Union[TextBlock, ToolUseBlock], PropertyInfo]], required=True), 'model': FieldInfo(annotation=Union[str, Literal['claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022', 'claude-3-5-sonnet-latest', 'claude-3-5-sonnet-20241022', 'claude-3-5-sonnet-20240620', 'claude-3-opus-latest', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307', 'claude-2.1', 'claude-2.0']], required=True), 'role': FieldInfo(annotation=Literal['assistant'], required=True), 'stop_reason': FieldInfo(annotation=Union[Literal['end_turn', 'max_tokens', 'stop_sequence', 'tool_use'], NoneType], required=False, default=None), 'stop_sequence': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'type': FieldInfo(annotation=Literal['message'], required=True), 'usage': FieldInfo(annotation=Usage, required=True)}\n",
      "model_fields_set: {'model', 'role', 'id', 'content', 'usage', 'stop_reason', 'type', 'stop_sequence'}\n",
      "model_json_schema: <bound method BaseModel.model_json_schema of <class 'anthropic.types.message.Message'>>\n",
      "model_parametrized_name: <bound method BaseModel.model_parametrized_name of <class 'anthropic.types.message.Message'>>\n",
      "model_post_init: <bound method BaseModel.model_post_init of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "model_rebuild: <bound method BaseModel.model_rebuild of <class 'anthropic.types.message.Message'>>\n",
      "model_validate: <bound method BaseModel.model_validate of <class 'anthropic.types.message.Message'>>\n",
      "model_validate_json: <bound method BaseModel.model_validate_json of <class 'anthropic.types.message.Message'>>\n",
      "model_validate_strings: <bound method BaseModel.model_validate_strings of <class 'anthropic.types.message.Message'>>\n",
      "parse_file: <bound method BaseModel.parse_file of <class 'anthropic.types.message.Message'>>\n",
      "parse_obj: <bound method BaseModel.parse_obj of <class 'anthropic.types.message.Message'>>\n",
      "parse_raw: <bound method BaseModel.parse_raw of <class 'anthropic.types.message.Message'>>\n",
      "role: assistant\n",
      "schema: <bound method BaseModel.schema of <class 'anthropic.types.message.Message'>>\n",
      "schema_json: <bound method BaseModel.schema_json of <class 'anthropic.types.message.Message'>>\n",
      "stop_reason: end_turn\n",
      "stop_sequence: None\n",
      "to_dict: <bound method BaseModel.to_dict of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "to_json: <bound method BaseModel.to_json of Message(id='msg_01JmHMthANCqW2tiLELDSecQ', content=[TextBlock(text=\"The capital of France is Paris.\\n\\nParis is the country's largest city and is located in the north-central part of France, in the region known as Île-de-France. It is situated on the River Seine and is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nAs the capital, Paris is the seat of France's national government and is home to the official residence of the President of the French Republic, the Élysée Palace. The city has played a significant role throughout French history and is a major global center for art, fashion, gastronomy, and culture.\", type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142))>\n",
      "type: message\n",
      "update_forward_refs: <bound method BaseModel.update_forward_refs of <class 'anthropic.types.message.Message'>>\n",
      "usage: Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=142)\n",
      "validate: <bound method BaseModel.validate of <class 'anthropic.types.message.Message'>>\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "\n",
    "def ask_claude(prompt):\n",
    "    # Initialize the Anthropic client with your API key\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    client = Anthropic()\n",
    "    \n",
    "    try:\n",
    "        # Create a message\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Print the entire raw response object\n",
    "        print(\"Raw Response Object:\")\n",
    "        print(message)\n",
    "        \n",
    "        # Print all attributes of the response\n",
    "        print(\"\\nAll Response Attributes:\")\n",
    "        for attr in dir(message):\n",
    "            if not attr.startswith('_'):  # Skip internal attributes\n",
    "                try:\n",
    "                    print(f\"{attr}: {getattr(message, attr)}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return message.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is the capital of France?\"\n",
    "    response = ask_claude(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.1-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.1-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 11.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.7/16.6 MB 11.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.8/16.6 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.9/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.5/16.6 MB 10.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.9/16.6 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.6 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 10.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"C:\\\\Users\\\\DNSOFT\\\\Downloads\\\\Receipt-2960-1788.pdf\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\DNSOFT\\\\Downloads\\\\Receipt-2960-1788.pdf', 'file_path': 'C:\\\\Users\\\\DNSOFT\\\\Downloads\\\\Receipt-2960-1788.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m96', 'creationDate': \"D:20240708085704+00'00'\", 'modDate': \"D:20240708085704+00'00'\", 'trapped': ''}, page_content='\\xa0\\n2960-1788 · $25.13 paid on July 8, 2024\\nPage 1 of 1\\nReceipt\\nInvoice number\\n1CA65F3F-0014\\nReceipt number\\n2960-1788\\nDate paid\\nJuly 8, 2024\\nPayment method Mastercard - 0835\\nOpenAI, LLC\\n548 Market Street\\nPMB 97273\\nSan Francisco, California 94104-5401\\nUnited States\\nar@openai.com\\nKR BRN 634-80-02934\\nBill to\\nJeon HyeonWoo\\n서울특별시 seoul\\nseoul\\nseoul\\n08511\\nSouth Korea\\nKR BRN 108-86-05434\\nShip to\\nJeon HyeonWoo\\n서울특별시 seoul\\nseoul\\nseoul\\n08511\\nSouth Korea\\n$25.13 paid on July 8, 2024\\nDescription\\nQty\\nUnit price\\nAmount\\nOpenAI API usage credit\\n1\\n$25.00\\n$25.00\\nSubtotal\\n$25.00\\nTax to be paid on reverse charge basis\\nTotal\\n$25.00\\nApplied balance\\n$0.13\\nAmount paid\\n$25.13\\n\\xa0\\n')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf4llm\n",
      "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pymupdf>=1.24.10 in c:\\users\\dnsoft\\test_notebooks_\\.venv\\lib\\site-packages (from pymupdf4llm) (1.25.1)\n",
      "Downloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pymupdf4llm\n",
      "Successfully installed pymupdf4llm-0.0.17\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf4llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\DNSOFT\\Downloads\\Receipt-2960-1788.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n"
     ]
    }
   ],
   "source": [
    "md_text = pymupdf4llm.to_markdown(\"C:\\\\Users\\\\DNSOFT\\\\Downloads\\\\Receipt-2960-1788.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# p\\n\\n**Invoice number** **1CA65F3F-0014**\\nReceipt number 2960-1788\\nDate paid July 8, 2024\\nPayment method Mastercard - 0835\\n\\n**OpenAI, LLC**\\n548 Market Street\\nPMB 97273\\nSan Francisco, California 94104-5401\\nUnited States\\nar@openai.com\\n\\nKR BRN 634-80-02934\\n\\n## $25.13 paid on July 8, 2024\\n\\n\\n**Bill to**\\nJeon HyeonWoo\\n서울특별시 seoul\\nseoul\\nseoul\\n08511\\nSouth Korea\\nKR BRN 108-86-05434\\n\\n\\n**Ship to**\\nJeon HyeonWoo\\n서울특별시 seoul\\nseoul\\nseoul\\n08511\\nSouth Korea\\n\\n\\nDescription Qty Unit price Amount\\n\\nOpenAI API usage credit 1 $25.00 $25.00\\n\\nSubtotal $25.00\\n\\nTax to be paid on reverse charge basis\\n\\nTotal $25.00\\n\\nApplied balance $0.13\\n\\n**Amount paid** **$25.13**\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\DNSOFT\\Downloads\\2감정-인식과-언어_최원일.pdf...\n",
      "[                                        ] (0/6=====[======                                  ] (1/6======[=============                           ] (2/======[====================                    ] (3/6=====[==========================              ] (4/6======[=================================       ] (5/======[========================================] (6/6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# 감정 지각 과정에서 언어의 역할\\n\\n광주과학기술원 최원일\\n\\n정보처리자로서의 인간의 삶에서 언어가 얼마나 중요한 역할을 하는가는 사실 말할 필요가\\n\\n없다(언어의 중요성에 관해 이야기하면서 ‘말할(언어를 사용할) 필요가 없다’라는 것이 좀 모\\n\\n순적이지만, 이 표현이 ‘그 정도로 중요하다’의 의미라는 것은 굳이 독자들께 설명해 드릴 필\\n\\n요가 없는 듯싶다. 앗. 또….). 이러한 언어의 중요성 때문에 인지과학 분야에서는 언어와 수\\n\\n개념의 관계, 언어와 색 개념의 관계와 같은 언어와 사고의 관계에 관한 연구가 꾸준하게 이\\n\\n루어져 왔다. 이 글에서는 언어와 감정의 관계, 조금 더 구체적으로 말하면, 언어가 타인의 감\\n\\n정을 지각하는 데 영향을 미치는가를 살펴보고 그 영향의 성격이 어떤 것인지 알아볼 것이다.\\n\\n먼저 언어와 사고의 관계에 관한 상당히 급진적인 주장인 벤자민 리 워프(Benjamin Lee\\n\\nWhorf)의 언어결정론에 관해 알아보자.\\n\\n**언어결정론: 언어 없이는 사고도 없다!?**\\n\\n언어결정론은 간단히 말하면 언어가 사고를 주도하고, 우리의 사고방식과 세상을 지각하는\\n\\n방식은 우리가 사용하는 언어에 의해 결정된다는 가설이다. 사실 언어가 사고에 미치는 영향\\n\\n에 관해서는 고대 그리스에서 활동했던 소피스트들의 주장까지 거슬러 올라갈 수 있으나, 언\\n\\n어 없이는 사고도 없다는 강한 언어결정론은 20세기 초반에 활동했던 벤자민 리 워프의 연구\\n\\n에서 시작되었다고 볼 수 있다. 언어결정론을 뒷받침한다고 알려진 한 예는 워프의 한 연구에\\n\\n서 찾아볼 수 있는데, 아메리카 원주민인 호피족이 사용하는 언어가 영어를 비롯한 유럽 언어\\n\\n가 시간을 묘사하는 방식과는 근본적인 차이가 있으며, 이러한 차이가 세계관의 차이를 야기\\n\\n한다는 것이다. 예를 들어 워프는 호피어에는 시간의 단위를 지칭하는 명사화된 개념이 없으\\n\\n며, 이는 호피족이 시간을 인식함에 있어 구분된 단위보다는 하나의 통합적인 과정으로 보도\\n\\n록 만든다는 것이다.[1)] 하지만 워프의 **강한 언어결정론은 이후 여러 학자들에게 그리고 다양한**\\n\\n측면에서 비판을 받아왔다. 지금은 워프가 주장했던 언어 없이는 사고도 없다는 강한 언어결\\n\\n정론은 받아들여지지 않는다. 하지만 언어가 인간의 정보처리 혹은 사고 과정에서 중요한 영\\n\\n향을 미칠 수 있다는 **약한 언어결정론은 다양한 경험 연구로부터 뒷받침되고 있으며 언어와**\\n\\n사고의 관계에 관하여 많은 학자들이 받아들이고 있는 관점이다.\\n\\n언어가 인간의 사고 과정에 영향을 미친다는 것을 인정한다면 인간이 감정을 지각하는 과\\n\\n정에서도 언어가 영향을 미칠 수 있는지, 있다면 그 영향은 얼마나 심대한가를 알아보는 것은\\n\\n상당히 흥미로운 질문이다. 언뜻 생각하면 감정을 지각하는 것은 상당히 자동적이고, 보편적\\n\\n인 과정이기에 언어가 미치는 영향이 있다 하더라도 크지 않을 것 같다. 자, 즉석 복권 백만\\n\\n원(로또 1등처럼 큰 당첨금을 받으면 어떤 감정이 생길지 도무지 감이 잡히지 않아 당첨금을\\n\\n백만 원으로 가정했다)에 당첨되어 활짝 웃는 표정이나, 화가 나는 일이 생겨 분노하는 표정\\n\\n을 알아차리기 위해 언어의 도움이 필요할까 생각해보자. 이러한 극단적인 케이스에서는 언어\\n\\n가 특별한 역할을 할 것 같지는 않다. 그냥 표정만 봐도 지금 그 사람이 어떤 감정인지 알아\\n\\n1) 호피어에 나타나는 시간 개념에 관한 논쟁에 관한 좀 더 자세한 설명을 원하는 독자라면 다음의 위키\\n\\n피디아 링크를 참조하기 바란다. https://en.wikipedia.org/wiki/Hopi_time_controversy\\n\\n\\n-----\\n\\n차릴 수 있지 않을까? 언어는 정말 감정 지각에 특별한 역할을 하는 것일까? 하지만 아래의\\n\\n‘그림 1‘을 한 번 들여다보자. 3명의 얼굴은 테니스 선수들이 경기 도중 아주 중요한 상황에\\n\\n서 득점을 했을 때와 실점을 했을 때의 표정을 보여주고 있다. 이 표정에 나타난 감정을 바탕\\n\\n으로 이것이 득점 상황인지 실점 상황인지 구분할 수 있겠는가? 만약 이 질문을 어렵다고 느\\n\\n꼈다면 여러분은 혼자가 아니다. 실제 Science에 발표된 Aviezer와 동료들(2012)의 연구에서\\n\\n도 사람들은 이 표정만 보고는 득실 상황을 정확히 구분할 수 없었다. 다시 말하면, 얼굴 표\\n\\n정만 가지고는 정확한 감정을 인식하기가 여간 어려운 일이 아니다. 이러한 관점에서 감정 지\\n\\n각에 관한 구성주의자들의 입장을 자세히 살펴보도록 하자.\\n\\n그림 1. HORIZON\\n\\n**감정 인식에 관한 구성주의 견해**\\n\\n구성주의 견해를 알아보기 전에 먼저 인간의 감정 인식에 관한 전통적인 입장을 간단하게\\n\\n살펴보자. 폴 에크만(Paul Ekman)은 분노, 공포, 슬픔, 기쁨, 놀람, 혐오와 같은 적어도 몇\\n\\n가지 기본 감정은 생애 초기에 나타나며, 얼굴 표정만 가지고도 이 감정들을 구분할 수 있고,\\n\\n생리적인 반응과 연결되어 있다고 주장하며, 이러한 감정들을 잘 인식하고 표현하는 것은 생\\n\\n존에 유용한 도구가 될 수 있기 때문에 인간의 언어, 민족, 문화에 관계없이 보편성을 갖는다\\n\\n고 말한다. 에크만의 감정에 관한 보편주의 견해는 1960년대 초기부터 큰 지지를 받아왔지만,\\n\\n이에 대한 비판 역시 꾸준하게 제기되어 왔다. 예를 들어 특정 감정을 표현하기 위한 얼굴 표\\n\\n정은 에크만이 제안한 것과 같이 일관된 얼굴 근육의 활동 단위를 사용하지 않기도 하고, 특\\n\\n정 감정과 특정 활동 단위의 조합이 일대일로 대응하는 것도 아니다. 이러한 비판의 중심에\\n\\n바로 감정 인식에 관한 구성주의 견해가 있다. 감정 인식의 구성주의 견해는 리사 펠드먼 배\\n\\n럿(Lisa F, Barret)과 동료들이 제안하였는데, 이들에 따르면 감정 인식을 위한 객관적 실체나\\n\\n단일한 요인이 있다기보다는 다양한 재료들이 상황에 맞게 뇌에서 구성된다고 한다. 예를 들\\n\\n어 앞에서 본 그림의 얼굴들을 떠올려 보자. 이들의 표정이 나타내는 감정을 파악하는 것이\\n\\n왜 쉽지 않았을까? 이것은 감정 지각이란 단순히 얼굴 근육의 여러 활동 단위 조합을 잘 읽\\n\\n어내는 일이 아니기 때문이다. 특정인의 표정에 나타난 감정을 제대로 파악하기 위해서는 그\\n\\n사람의 표정과 함께 당시의 상황 문맥을 정확하게 이해해야 한다. 배럿의 말을 빌리자면, 당\\n\\n시의 사회적 상황, 몸의 자세, 목소리, 장면, 심지어 다른 사람들의 표정까지도 감정 지각의\\n\\n재료가 된다(Barret, Lindquist, & Gendron, 2007).\\n\\n구성주의 견해에서 감정 지각의 중요한 재료 중 특별한 역할을 한다고 생각하는 것이 바로\\n\\n언어이다. 우리가 앞서 살펴본 약한 언어결정론에 따르면, 인간의 언어는 다양한 정보처리 과\\n\\n\\n-----\\n\\n정에 큰 영향을 미칠 수 있다. 따라서 언어가 감정을 인식하는 과정에서도 고유한 맥락을 제\\n\\n공한다는 것이다. 실제로 여러 인지과학 기반의 경험 연구에서는 사람들이 구조적 유사성이\\n\\n거의 없는 새로운 사물들의 관계에 관한 개념적 추론을 할 때 단어를 이용한다는 것을 보여주\\n\\n었다. 감정과 같은 범주에서도 언어는 유사한 역할을 할 수 있다. Lindquist와\\n\\nGendron(2013)이 설명한 바와 같이, 친구에게 얼굴을 찡그리거나, 주차 위반 고지서를 받아\\n\\n서 들고 뿌루퉁한 표정을 짓거나, 모욕당해 속이 부글거리거나, 심지어 잘못한 아이를 보면서\\n\\n미소를 보일 때조차도 얼굴에 나타난 표정은 조금씩 다르겠지만, 이를 모두 분노라는 감정의\\n\\n예시라 할 수 있는데, 이것은 바로 **분노라는 단어가 분노 감정을 지각하도록 하는 맥락을 제**\\n\\n공했기 때문이라는 것이다. 따라서 구성주의 견해에서는 특정 상황에 나타나는 감정을 지각하\\n\\n거나 경험할 때 언어는 필수적인 역할 혹은 예측력을 가지며, 특히 감정 단어(그리고 이와 연\\n\\n결된 개념 지식)는 감정을 지각하고 경험하는 데 없어서는 안 될 재료라고 본다. 그러면 이러\\n\\n한 입장을 지지하는 경험 연구 결과를 살펴보도록 하자.\\n\\n**구성주의 견해를 지지하는 경험 연구 결과**\\n\\n먼저 뇌 영상 연구 결과를 전반적으로 살펴보면, 감정 지각 및 경험과 관련되는 개별 뇌 영\\n\\n상 연구를 메타 분석한 결과 감정 지각과 관련되어 있다고 알려진 뇌 영역인 편도체\\n\\n(amygdala)나 뇌섬엽(insula)은 언어 및 개념 정보의 처리와 연관된 뇌 영역이라고 알려진 복\\n\\n외측 전전두피질(vlPFC)이나 하측 측두피질(ITC), 배내측 전전두피질(dmPFC), 전측 측두 영\\n\\n역(ATL)과 서로 긴밀하게 상호작용하며 연결되어 있다고 한다(Lindquist et al., 2015). 예를\\n\\n들어 배내측 전전두피질은 개념 지식에 접근하고 이를 통합하는 것과 관련된 뇌 영역인데, 이\\n\\n는 특정 자극으로부터 유발된 감정 상태가 의미하는 바가 무엇인지 파악하는 것과 연관되어\\n\\n있으며, 복외측 전전두피질은 감정 경험을 범주화하기 위한 의미 지식을 활성화하는 것에 관\\n\\n여할 수 있다. 그리고 최근 발표된 한 뇌 영상 연구에 따르면 특정 표정에 특정 감정 범주의\\n\\n이름을 붙이는 것은 감정 지각과 관련된 뇌 영역인 편도체와 뇌섬엽의 활성화를 증가시키고,\\n\\n편도체와 복내측 전전두피질의 연결을 더욱 강화하는 결과를 산출했다(Sapute et al., 2016).\\n\\n이는 언어를 이용한 어휘 표지(labeling)가 실제로 정서 반응의 강도를 변화시킬 수 있음을 시\\n\\n사한다.\\n\\n앞서 살펴본 뇌 영상 연구 결과가 시사하는 바는 전두-측두 신경 인지 장애로 인한 언어\\n\\n손상 환자의 사례를 통해서도 뒷받침될 수 있다. Lindquist와 동료들이 2014년 발표한 연구\\n\\n에 따르면 이러한 언어 손상 환자들이 표정에 나타난 감정을 범주화하는 과제 수행 시 연령대\\n\\n가 비슷한 건강한 통제 집단과는 질적으로 다른 범주화 양상을 보였다고 한다. 언어 손상 환\\n\\n자들은 통제 집단보다 훨씬 적은 수의 범주를 만들어냈고, 특히 부정 감정(negative\\n\\nemotion)을 나타내는 표정의 사진을 범주화할 때 큰 어려움을 나타내었다(Lindquist et al.,\\n\\n2014). 이러한 결과는 손상된 언어 능력으로 인하여 표정에 나타난 감정을 정확하게 범주화하\\n\\n는 데 어려움을 겪는 것으로 해석할 수 있다.\\n\\n마지막으로 전통적인 실험심리학 연구에 기반한 최신 증거를 하나 더 알아보자. Doyle과\\n\\n동료들은 2021년 매우 흥미로운 연구 결과를 발표하였다. 이 연구에서는 ’그림 2‘에 나타난\\n\\n바와 같이 점화 패러다임을 사용하였는데, 세 가지 서로 다른 점화 자극이 화면에 제시되었\\n\\n다. 하나는 **감정 어휘였고, 다른 하나는 그러한 감정과 관련된 상황을 나타내는** **장면 사진이**\\n\\n었으며, 다른 하나는 아무 정보도 제시되지 않는 **하얀 화면이었다. 이 자극이 250밀리초 동안**\\n\\n\\n-----\\n\\n제시되고 그 후 바로 목표 자극이 제시되는데, 특정 감정을 표현하는 얼굴이 300밀리초 동안\\n\\n화면에 머무른다. 그 뒤에 두 개의 표정을 제시하여 방금 본 목표 자극과 같은 자극이 무엇인\\n\\n지 연구 대상자가 선택하는 과제였다. 만약 언어로 제시된 점화 자극이 목표 자극으로 제시된\\n\\n표정이 어떤 감정인지 지각하는 데 도움이 된다면, 그 뒤 같은 표정을 선택하는 과제에서 다\\n\\n른 점화 자극이 사용된 조건들보다 더 나은 수행을 보이는 것을 기대할 수 있다. 실제 결과\\n\\n역시 예상대로 나타났다. 감정 어휘가 점화 자극으로 사용되었을 때가 다른 조건에 비해 과제\\n\\n수행의 정확도가 더 높았다(Doyle et al., 2021). 구성주의 견해에 따르면 이러한 결과는 언어\\n\\n가 감정 지각을 촉진하는 특별한 맥락을 제공하는 재료임을 시사한다.\\n\\n그림 2. Yongss, 사진(Homeless man in NYC-Linda FletcherⓒWikimedia Commons, 표정들-루치아)\\n\\n지금까지 살펴본 경험 연구 결과는 언어가 감정 지각 및 경험을 위한 구별된 기능을 한다는\\n\\n구성주의의 견해를 뒷받침하는 것으로 해석할 수 있다. 하지만 이러한 결론을 내리기에 앞서\\n\\n이 절에서 소개한 경험 연구 결과들이 의미하는 바에 관하여 조금 더 생각해보도록 하자.\\n\\n**구성주의 견해에 관한 비판적 검토**\\n\\n앞 절에서 설명한 경험 연구 결과는 감정 지각과 경험에 관한 구성주의 견해를 분명히 지지\\n\\n한다. 그러나 이러한 결과를 해석하면서 몇 가지 유의할 점을 생각하고자 한다.\\n\\n첫째, 앞서 살펴본 Lindquist와 동료들의 뇌 영상 연구 결과는 분명 감정을 지각할 때 감정\\n\\n관련 영역과 함께 의미 관련 영역도 활성화되며, 이 두 영역의 활성화가 동기화된다는 것을\\n\\n보여주었다. 하지만 이러한 결과가 마치 언어 정보의 활성화가 선행되어야만, 원인이 되어 감\\n\\n정을 지각하게 된다는 식으로 해석되어서는 안 된다. 뇌 영상 결과는 정보처리의 상관물일\\n\\n뿐, 원인이 아니다. 예를 들어 울고 있는 표정에 나타난 감정을 먼저 지각한 뒤, 이와 관련된\\n\\n의미 정보나 개념 지식이 나중에 활성화될 수도 있는 것이다. 이러한 주장에 대하여, 앞서 소\\n\\n개한 언어 손상 환자가 감정이 표현된 얼굴 사진들을 정확하게 범주화해내지 못한 연구 결과\\n\\n(Lindquist et al., 2014)를 떠올리는 독자가 계실지도 모르겠다. 언어가 손상된 환자가 감정\\n\\n의 범주화를 정확하게 못 하는 것을 보면, 언어가 감정 지각의 중요한 원인이 될 수 있는 것\\n\\n아닌가? 물론 그렇다. 하지만 이 연구 하나만 가지고 그런 결론을 내리는 것은 위험하다. 소\\n\\n수의 환자를 대상으로 한 신경심리 연구에서는 환자 개개인이 가진 고유성이 결과에 영향을\\n\\n미칠 수 있다는 한계가 있으며, 앞서 소개한 연구 자체의 한계도 존재했다. 예를 들어 이 연\\n\\n\\n-----\\n\\n구에 사용된 표정 사진들은 원래 여섯 개의 범주에서 가져왔는데, 건강한 통제 집단의 실험\\n\\n참여자들은 이 사진들을 약 8개의 집단으로 범주화했다. 실험을 위해 선택된 사진이 가진 특\\n\\n징이든, 실험 자체가 가지는 독특한 특징이 이러한 결과를 만들었을 가능성이 있다. 이는 이\\n\\n러한 연구들이 다양한 측면에서 재연될 필요가 있음을 시사한다.\\n\\n둘째, 감정 어휘가 다른 자극에 비해 감정 지각을 더 효율적으로 만든다는 결과를 얻은\\n\\nDoyle과 동료들의 연구 역시 비판적인 시각에서 검토할 수 있다. 가장 먼저 제기할 수 있는\\n\\n문제는 이 결과가 언어라는 혹은 언어가 제공하는 특별한 의미 맥락이 감정을 지각하는 데 꼭\\n\\n필요한 재료임을 보여주는가이다. 물론 언어 자극이 과제에 앞서 제시되었을 때 가장 정확한\\n\\n감정 지각 결과가 나타나긴 했지만, 다른 조건에서 감정 지각이 전혀 불가능하거나 정확도가\\n\\n현격히 떨어진 것은 아니었다. 실제 조건 간 정확도의 차이는 실험에 따라 약간 달랐지만 대\\n\\n략 2~4% 정도였다. 이 결과가 언어가 특정한 상황 아래의 감정 지각에 유의미한 도움을 줄\\n\\n수 있는 것은 맞지만, 저자들의 주장처럼 언어가 꼭 필요한 맥락으로 기능하는가는 조금 더\\n\\n깊이 고민해봐야 할 문제이다.\\n\\n그렇다면 구성주의 견해에서 주장하는 바와 같이 언어가 감정 지각을 위해 빼놓을 수 없는\\n\\n재료임을 시사하는 보다 직접적인 증거는 어떤 것이 될 수 있을까? 앞서 언급한 바와 같이\\n\\n언어 능력이나 개념 지식만이 심각하게 손상된 환자의 감정 지각 능력이 현저히 저하된 결과\\n\\n를 볼 수 있다면 감정 지각에 언어가 심대한 관여를 한다는 직접적인 증거가 될 수 있을 것이\\n\\n다. 또한 한 언어에서 특정 감정을 나타내는 단어가 없을 때, 해당 감정의 지각과 경험이 유\\n\\n의미하게 영향을 받는다면 이 또한 언어가 감정 지각의 필수적인 재료임을 보여주는 하나의\\n\\n예시가 될 수 있을 것이다. 2006년 Breugelmans와 Poortinga는 이러한 주제로 흥미로운 연\\n\\n구 결과 하나를 발표한다. 이들에 따르면 멕시코 지역의 원주민인 라라무리(Rarámuri) 인디\\n\\n언 언어에는 죄책감과 수치심(혹은 창피함)을 나타내는 단어가 하나뿐이다(논문에서는 이를\\n\\nshame으로 번역함).[2)] 많은 연구가 죄책감과 수치심이 서로 다른 자의식 감정이라고 보고하고\\n\\n있는데, 그렇다면 라라무리 인디언들은 이 두 감정을 하나의 감정으로 인식할까? 이 논문의\\n\\n저자들은 이를 알아보기 위해 라라무리 원주민을 대상으로 평정 연구를 하였다. 먼저 이 원주\\n\\n민들에게 다양한 감정을 유발하는 상황에 관한 자료를 얻은 뒤, 네덜란드와 인도네시아의 대\\n\\n학생들에게 이 자료가 묘사하는 상황이 어떤 감정을 유발하는가 평정하도록 하였다. 그 결과\\n\\n죄책감/수치심과 관련된 18개의 상황을 설정할 수 있었고, 라라무리 인디언들이 이 상황들을\\n\\n죄책감과 수치심의 차원으로 각각 구분할 수 있는가를 평정을 통해 알아보았다. 그 결과, 비\\n\\n록 죄책감에 해당하는 단어가 존재하지 않는 라라무리 인디언들도 이 두 감정을 개념적으로\\n\\n잘 구분하는 것으로 나타났다. 물론 이 연구 결과가 감정 지각 및 경험에 언어의 역할이 없다\\n\\n고 말하지는 않는다. 하지만 적어도 죄책감이라는 단어가 없다고 해서 죄책감이라는 감정을\\n\\n수치심과 구분하지 못하는 것은 아니라는 점을 분명하게 보여주고 있다.\\n\\n지금까지 우리는 감정 지각과 감정 경험에서 언어라는 재료를 어떻게 바라봐야 하는가를 살\\n\\n펴보았다. 약한 언어결정론에서 본 바와 같이 언어는 분명 인간의 다양한 정보처리 과정에서\\n\\n상당히 중요한 영향을 미친다. 당연히 감정 경험의 전 과정 중 일정 부분에서 언어가 역할을\\n\\n한다는 것에는 어느 누구도 이의를 제기하지 않을 것이다. 감정에 관한 구성주의 견해 역시\\n\\n언어가 감정 지각을 위한 고유한 맥락을 제공하는 핵심 재료라고 주장하며 이를 지지하는 여\\n\\n러 경험 연구 결과를 보여 주었다. 하지만 우리가 이 절에서 검토한 바와 같이 감정 지각을\\n\\n2) 수치심으로 번역되는 라라무리어의 단어는 굳이 쓰자면 riwérama라고 쓸 수 있는데, 실제로는 이 언\\n\\n어는 문자가 없어서 발음상 다양한 변이가 존재한다고 한다.\\n\\n\\n-----\\n\\n위해 언어가 **어떤 역할을 하는가에 관해서는 보다 신중한 접근이 필요하다. 자칫 잘못하면 언**\\n\\n어 없이는 감정도 없다는 식의 극단적 방식으로 워프의 부활이 일어날 수 있으니 말이다.\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_text = pymupdf4llm.to_markdown(\"C:\\\\Users\\\\DNSOFT\\\\Downloads\\\\2감정-인식과-언어_최원일.pdf\")\n",
    "md_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 감정 지각 과정에서 언어의 역할\\n\\n광주과학기술원 최원일\\n\\n정보처리자로서의 인간의 삶에서 언어가 얼마나 중요한 역할을 하는가는 사실 말할 필요가\\n\\n없다(언어의 중요성에 관해 이야기하면서 ‘말할(언어를 사용할) 필요가 없다’라는 것이 좀 모\\n\\n순적이지만, 이 표현이 ‘그 정도로 중요하다’의 의미라는 것은 굳이 독자들께 설명해 드릴 필\\n\\n요가 없는 듯싶다. 앗. 또….). 이러한 언어의 중요성 때문에 인지과학 분야에서는 언어와 수\\n\\n개념의 관계, 언어와 색 개념의 관계와 같은 언어와 사고의 관계에 관한 연구가 꾸준하게 이\\n\\n루어져 왔다. 이 글에서는 언어와 감정의 관계, 조금 더 구체적으로 말하면, 언어가 타인의 감\\n\\n정을 지각하는 데 영향을 미치는가를 살펴보고 그 영향의 성격이 어떤 것인지 알아볼 것이다.\\n\\n먼저 언어와 사고의 관계에 관한 상당히 급진적인 주장인 벤자민 리 워프(Benjamin Lee\\n\\nWhorf)의 언어결정론에 관해 알아보자.\\n\\n**언어결정론: 언어 없이는 사고도 없다!?**\\n\\n언어결정론은 간단히 말하면 언어가 사고를 주도하고, 우리의 사고방식과 세상을 지각하는\\n\\n방식은 우리가 사용하는 언어에 의해 결정된다는 가설이다. 사실 언어가 사고에 미치는 영향\\n\\n에 관해서는 고대 그리스에서 활동했던 소피스트들의 주장까지 거슬러 올라갈 수 있으나, 언\\n\\n어 없이는 사고도 없다는 강한 언어결정론은 20세기 초반에 활동했던 벤자민 리 워프의 연구\\n\\n에서 시작되었다고 볼 수 있다. 언어결정론을 뒷받침한다고 알려진 한 예는 워프의 한 연구에\\n\\n서 찾아볼 수 있는데, 아메리카 원주민인 호피족이 사용하는 언어가 영어를 비롯한 유럽 언어\\n\\n가 시간을 묘사하는 방식과는 근본적인 차이가 있으며, 이러한 차이가 세계관의 차이를 야기\\n\\n한다는 것이다. 예를 들어 워프는 호피어에는 시간의 단위를 지칭하는 명사화된 개념이 없으\\n\\n며, 이는 호피족이 시간을 인식함에 있어 구분된 단위보다는 하나의 통합적인 과정으로 보도\\n\\n록 만든다는 것이다.[1)] 하지만 워프의 **강한 언어결정론은 이후 여러 학자들에게 그리고 다양한**\\n\\n측면에서 비판을 받아왔다. 지금은 워프가 주장했던 언어 없이는 사고도 없다는 강한 언어결\\n\\n정론은 받아들여지지 않는다. 하지만 언어가 인간의 정보처리 혹은 사고 과정에서 중요한 영\\n\\n향을 미칠 수 있다는 **약한 언어결정론은 다양한 경험 연구로부터 뒷받침되고 있으며 언어와**\\n\\n사고의 관계에 관하여 많은 학자들이 받아들이고 있는 관점이다.\\n\\n언어가 인간의 사고 과정에 영향을 미친다는 것을 인정한다면 인간이 감정을 지각하는 과\\n\\n정에서도 언어가 영향을 미칠 수 있는지, 있다면 그 영향은 얼마나 심대한가를 알아보는 것은\\n\\n상당히 흥미로운 질문이다. 언뜻 생각하면 감정을 지각하는 것은 상당히 자동적이고, 보편적\\n\\n인 과정이기에 언어가 미치는 영향이 있다 하더라도 크지 않을 것 같다. 자, 즉석 복권 백만\\n\\n원(로또 1등처럼 큰 당첨금을 받으면 어떤 감정이 생길지 도무지 감이 잡히지 않아 당첨금을\\n\\n백만 원으로 가정했다)에 당첨되어 활짝 웃는 표정이나, 화가 나는 일이 생겨 분노하는 표정\\n\\n을 알아차리기 위해 언어의 도움이 필요할까 생각해보자. 이러한 극단적인 케이스에서는 언어\\n\\n가 특별한 역할을 할 것 같지는 않다. 그냥 표정만 봐도 지금 그 사람이 어떤 감정인지 알아\\n\\n1) 호피어에 나타나는 시간 개념에 관한 논쟁에 관한 좀 더 자세한 설명을 원하는 독자라면 다음의 위키\\n\\n피디아 링크를 참조하기 바란다. https://en.wikipedia.org/wiki/Hopi_time_controversy\\n\\n\\n-----\\n\\n차릴 수 있지 않을까? 언어는 정말 감정 지각에 특별한 역할을 하는 것일까? 하지만 아래의\\n\\n‘그림 1‘을 한 번 들여다보자. 3명의 얼굴은 테니스 선수들이 경기 도중 아주 중요한 상황에\\n\\n서 득점을 했을 때와 실점을 했을 때의 표정을 보여주고 있다. 이 표정에 나타난 감정을 바탕\\n\\n으로 이것이 득점 상황인지 실점 상황인지 구분할 수 있겠는가? 만약 이 질문을 어렵다고 느\\n\\n꼈다면 여러분은 혼자가 아니다. 실제 Science에 발표된 Aviezer와 동료들(2012)의 연구에서\\n\\n도 사람들은 이 표정만 보고는 득실 상황을 정확히 구분할 수 없었다. 다시 말하면, 얼굴 표\\n\\n정만 가지고는 정확한 감정을 인식하기가 여간 어려운 일이 아니다. 이러한 관점에서 감정 지\\n\\n각에 관한 구성주의자들의 입장을 자세히 살펴보도록 하자.\\n\\n그림 1. HORIZON\\n\\n**감정 인식에 관한 구성주의 견해**\\n\\n구성주의 견해를 알아보기 전에 먼저 인간의 감정 인식에 관한 전통적인 입장을 간단하게\\n\\n살펴보자. 폴 에크만(Paul Ekman)은 분노, 공포, 슬픔, 기쁨, 놀람, 혐오와 같은 적어도 몇\\n\\n가지 기본 감정은 생애 초기에 나타나며, 얼굴 표정만 가지고도 이 감정들을 구분할 수 있고,\\n\\n생리적인 반응과 연결되어 있다고 주장하며, 이러한 감정들을 잘 인식하고 표현하는 것은 생\\n\\n존에 유용한 도구가 될 수 있기 때문에 인간의 언어, 민족, 문화에 관계없이 보편성을 갖는다\\n\\n고 말한다. 에크만의 감정에 관한 보편주의 견해는 1960년대 초기부터 큰 지지를 받아왔지만,\\n\\n이에 대한 비판 역시 꾸준하게 제기되어 왔다. 예를 들어 특정 감정을 표현하기 위한 얼굴 표\\n\\n정은 에크만이 제안한 것과 같이 일관된 얼굴 근육의 활동 단위를 사용하지 않기도 하고, 특\\n\\n정 감정과 특정 활동 단위의 조합이 일대일로 대응하는 것도 아니다. 이러한 비판의 중심에\\n\\n바로 감정 인식에 관한 구성주의 견해가 있다. 감정 인식의 구성주의 견해는 리사 펠드먼 배\\n\\n럿(Lisa F, Barret)과 동료들이 제안하였는데, 이들에 따르면 감정 인식을 위한 객관적 실체나\\n\\n단일한 요인이 있다기보다는 다양한 재료들이 상황에 맞게 뇌에서 구성된다고 한다. 예를 들\\n\\n어 앞에서 본 그림의 얼굴들을 떠올려 보자. 이들의 표정이 나타내는 감정을 파악하는 것이\\n\\n왜 쉽지 않았을까? 이것은 감정 지각이란 단순히 얼굴 근육의 여러 활동 단위 조합을 잘 읽\\n\\n어내는 일이 아니기 때문이다. 특정인의 표정에 나타난 감정을 제대로 파악하기 위해서는 그\\n\\n사람의 표정과 함께 당시의 상황 문맥을 정확하게 이해해야 한다. 배럿의 말을 빌리자면, 당\\n\\n시의 사회적 상황, 몸의 자세, 목소리, 장면, 심지어 다른 사람들의 표정까지도 감정 지각의\\n\\n재료가 된다(Barret, Lindquist, & Gendron, 2007).\\n\\n구성주의 견해에서 감정 지각의 중요한 재료 중 특별한 역할을 한다고 생각하는 것이 바로\\n\\n언어이다. 우리가 앞서 살펴본 약한 언어결정론에 따르면, 인간의 언어는 다양한 정보처리 과\\n\\n\\n-----\\n\\n정에 큰 영향을 미칠 수 있다. 따라서 언어가 감정을 인식하는 과정에서도 고유한 맥락을 제\\n\\n공한다는 것이다. 실제로 여러 인지과학 기반의 경험 연구에서는 사람들이 구조적 유사성이\\n\\n거의 없는 새로운 사물들의 관계에 관한 개념적 추론을 할 때 단어를 이용한다는 것을 보여주\\n\\n었다. 감정과 같은 범주에서도 언어는 유사한 역할을 할 수 있다. Lindquist와\\n\\nGendron(2013)이 설명한 바와 같이, 친구에게 얼굴을 찡그리거나, 주차 위반 고지서를 받아\\n\\n서 들고 뿌루퉁한 표정을 짓거나, 모욕당해 속이 부글거리거나, 심지어 잘못한 아이를 보면서\\n\\n미소를 보일 때조차도 얼굴에 나타난 표정은 조금씩 다르겠지만, 이를 모두 분노라는 감정의\\n\\n예시라 할 수 있는데, 이것은 바로 **분노라는 단어가 분노 감정을 지각하도록 하는 맥락을 제**\\n\\n공했기 때문이라는 것이다. 따라서 구성주의 견해에서는 특정 상황에 나타나는 감정을 지각하\\n\\n거나 경험할 때 언어는 필수적인 역할 혹은 예측력을 가지며, 특히 감정 단어(그리고 이와 연\\n\\n결된 개념 지식)는 감정을 지각하고 경험하는 데 없어서는 안 될 재료라고 본다. 그러면 이러\\n\\n한 입장을 지지하는 경험 연구 결과를 살펴보도록 하자.\\n\\n**구성주의 견해를 지지하는 경험 연구 결과**\\n\\n먼저 뇌 영상 연구 결과를 전반적으로 살펴보면, 감정 지각 및 경험과 관련되는 개별 뇌 영\\n\\n상 연구를 메타 분석한 결과 감정 지각과 관련되어 있다고 알려진 뇌 영역인 편도체\\n\\n(amygdala)나 뇌섬엽(insula)은 언어 및 개념 정보의 처리와 연관된 뇌 영역이라고 알려진 복\\n\\n외측 전전두피질(vlPFC)이나 하측 측두피질(ITC), 배내측 전전두피질(dmPFC), 전측 측두 영\\n\\n역(ATL)과 서로 긴밀하게 상호작용하며 연결되어 있다고 한다(Lindquist et al., 2015). 예를\\n\\n들어 배내측 전전두피질은 개념 지식에 접근하고 이를 통합하는 것과 관련된 뇌 영역인데, 이\\n\\n는 특정 자극으로부터 유발된 감정 상태가 의미하는 바가 무엇인지 파악하는 것과 연관되어\\n\\n있으며, 복외측 전전두피질은 감정 경험을 범주화하기 위한 의미 지식을 활성화하는 것에 관\\n\\n여할 수 있다. 그리고 최근 발표된 한 뇌 영상 연구에 따르면 특정 표정에 특정 감정 범주의\\n\\n이름을 붙이는 것은 감정 지각과 관련된 뇌 영역인 편도체와 뇌섬엽의 활성화를 증가시키고,\\n\\n편도체와 복내측 전전두피질의 연결을 더욱 강화하는 결과를 산출했다(Sapute et al., 2016).\\n\\n이는 언어를 이용한 어휘 표지(labeling)가 실제로 정서 반응의 강도를 변화시킬 수 있음을 시\\n\\n사한다.\\n\\n앞서 살펴본 뇌 영상 연구 결과가 시사하는 바는 전두-측두 신경 인지 장애로 인한 언어\\n\\n손상 환자의 사례를 통해서도 뒷받침될 수 있다. Lindquist와 동료들이 2014년 발표한 연구\\n\\n에 따르면 이러한 언어 손상 환자들이 표정에 나타난 감정을 범주화하는 과제 수행 시 연령대\\n\\n가 비슷한 건강한 통제 집단과는 질적으로 다른 범주화 양상을 보였다고 한다. 언어 손상 환\\n\\n자들은 통제 집단보다 훨씬 적은 수의 범주를 만들어냈고, 특히 부정 감정(negative\\n\\nemotion)을 나타내는 표정의 사진을 범주화할 때 큰 어려움을 나타내었다(Lindquist et al.,\\n\\n2014). 이러한 결과는 손상된 언어 능력으로 인하여 표정에 나타난 감정을 정확하게 범주화하\\n\\n는 데 어려움을 겪는 것으로 해석할 수 있다.\\n\\n마지막으로 전통적인 실험심리학 연구에 기반한 최신 증거를 하나 더 알아보자. Doyle과\\n\\n동료들은 2021년 매우 흥미로운 연구 결과를 발표하였다. 이 연구에서는 ’그림 2‘에 나타난\\n\\n바와 같이 점화 패러다임을 사용하였는데, 세 가지 서로 다른 점화 자극이 화면에 제시되었\\n\\n다. 하나는 **감정 어휘였고, 다른 하나는 그러한 감정과 관련된 상황을 나타내는** **장면 사진이**\\n\\n었으며, 다른 하나는 아무 정보도 제시되지 않는 **하얀 화면이었다. 이 자극이 250밀리초 동안**\\n\\n\\n-----\\n\\n제시되고 그 후 바로 목표 자극이 제시되는데, 특정 감정을 표현하는 얼굴이 300밀리초 동안\\n\\n화면에 머무른다. 그 뒤에 두 개의 표정을 제시하여 방금 본 목표 자극과 같은 자극이 무엇인\\n\\n지 연구 대상자가 선택하는 과제였다. 만약 언어로 제시된 점화 자극이 목표 자극으로 제시된\\n\\n표정이 어떤 감정인지 지각하는 데 도움이 된다면, 그 뒤 같은 표정을 선택하는 과제에서 다\\n\\n른 점화 자극이 사용된 조건들보다 더 나은 수행을 보이는 것을 기대할 수 있다. 실제 결과\\n\\n역시 예상대로 나타났다. 감정 어휘가 점화 자극으로 사용되었을 때가 다른 조건에 비해 과제\\n\\n수행의 정확도가 더 높았다(Doyle et al., 2021). 구성주의 견해에 따르면 이러한 결과는 언어\\n\\n가 감정 지각을 촉진하는 특별한 맥락을 제공하는 재료임을 시사한다.\\n\\n그림 2. Yongss, 사진(Homeless man in NYC-Linda FletcherⓒWikimedia Commons, 표정들-루치아)\\n\\n지금까지 살펴본 경험 연구 결과는 언어가 감정 지각 및 경험을 위한 구별된 기능을 한다는\\n\\n구성주의의 견해를 뒷받침하는 것으로 해석할 수 있다. 하지만 이러한 결론을 내리기에 앞서\\n\\n이 절에서 소개한 경험 연구 결과들이 의미하는 바에 관하여 조금 더 생각해보도록 하자.\\n\\n**구성주의 견해에 관한 비판적 검토**\\n\\n앞 절에서 설명한 경험 연구 결과는 감정 지각과 경험에 관한 구성주의 견해를 분명히 지지\\n\\n한다. 그러나 이러한 결과를 해석하면서 몇 가지 유의할 점을 생각하고자 한다.\\n\\n첫째, 앞서 살펴본 Lindquist와 동료들의 뇌 영상 연구 결과는 분명 감정을 지각할 때 감정\\n\\n관련 영역과 함께 의미 관련 영역도 활성화되며, 이 두 영역의 활성화가 동기화된다는 것을\\n\\n보여주었다. 하지만 이러한 결과가 마치 언어 정보의 활성화가 선행되어야만, 원인이 되어 감\\n\\n정을 지각하게 된다는 식으로 해석되어서는 안 된다. 뇌 영상 결과는 정보처리의 상관물일\\n\\n뿐, 원인이 아니다. 예를 들어 울고 있는 표정에 나타난 감정을 먼저 지각한 뒤, 이와 관련된\\n\\n의미 정보나 개념 지식이 나중에 활성화될 수도 있는 것이다. 이러한 주장에 대하여, 앞서 소\\n\\n개한 언어 손상 환자가 감정이 표현된 얼굴 사진들을 정확하게 범주화해내지 못한 연구 결과\\n\\n(Lindquist et al., 2014)를 떠올리는 독자가 계실지도 모르겠다. 언어가 손상된 환자가 감정\\n\\n의 범주화를 정확하게 못 하는 것을 보면, 언어가 감정 지각의 중요한 원인이 될 수 있는 것\\n\\n아닌가? 물론 그렇다. 하지만 이 연구 하나만 가지고 그런 결론을 내리는 것은 위험하다. 소\\n\\n수의 환자를 대상으로 한 신경심리 연구에서는 환자 개개인이 가진 고유성이 결과에 영향을\\n\\n미칠 수 있다는 한계가 있으며, 앞서 소개한 연구 자체의 한계도 존재했다. 예를 들어 이 연\\n\\n\\n-----\\n\\n구에 사용된 표정 사진들은 원래 여섯 개의 범주에서 가져왔는데, 건강한 통제 집단의 실험\\n\\n참여자들은 이 사진들을 약 8개의 집단으로 범주화했다. 실험을 위해 선택된 사진이 가진 특\\n\\n징이든, 실험 자체가 가지는 독특한 특징이 이러한 결과를 만들었을 가능성이 있다. 이는 이\\n\\n러한 연구들이 다양한 측면에서 재연될 필요가 있음을 시사한다.\\n\\n둘째, 감정 어휘가 다른 자극에 비해 감정 지각을 더 효율적으로 만든다는 결과를 얻은\\n\\nDoyle과 동료들의 연구 역시 비판적인 시각에서 검토할 수 있다. 가장 먼저 제기할 수 있는\\n\\n문제는 이 결과가 언어라는 혹은 언어가 제공하는 특별한 의미 맥락이 감정을 지각하는 데 꼭\\n\\n필요한 재료임을 보여주는가이다. 물론 언어 자극이 과제에 앞서 제시되었을 때 가장 정확한\\n\\n감정 지각 결과가 나타나긴 했지만, 다른 조건에서 감정 지각이 전혀 불가능하거나 정확도가\\n\\n현격히 떨어진 것은 아니었다. 실제 조건 간 정확도의 차이는 실험에 따라 약간 달랐지만 대\\n\\n략 2~4% 정도였다. 이 결과가 언어가 특정한 상황 아래의 감정 지각에 유의미한 도움을 줄\\n\\n수 있는 것은 맞지만, 저자들의 주장처럼 언어가 꼭 필요한 맥락으로 기능하는가는 조금 더\\n\\n깊이 고민해봐야 할 문제이다.\\n\\n그렇다면 구성주의 견해에서 주장하는 바와 같이 언어가 감정 지각을 위해 빼놓을 수 없는\\n\\n재료임을 시사하는 보다 직접적인 증거는 어떤 것이 될 수 있을까? 앞서 언급한 바와 같이\\n\\n언어 능력이나 개념 지식만이 심각하게 손상된 환자의 감정 지각 능력이 현저히 저하된 결과\\n\\n를 볼 수 있다면 감정 지각에 언어가 심대한 관여를 한다는 직접적인 증거가 될 수 있을 것이\\n\\n다. 또한 한 언어에서 특정 감정을 나타내는 단어가 없을 때, 해당 감정의 지각과 경험이 유\\n\\n의미하게 영향을 받는다면 이 또한 언어가 감정 지각의 필수적인 재료임을 보여주는 하나의\\n\\n예시가 될 수 있을 것이다. 2006년 Breugelmans와 Poortinga는 이러한 주제로 흥미로운 연\\n\\n구 결과 하나를 발표한다. 이들에 따르면 멕시코 지역의 원주민인 라라무리(Rarámuri) 인디\\n\\n언 언어에는 죄책감과 수치심(혹은 창피함)을 나타내는 단어가 하나뿐이다(논문에서는 이를\\n\\nshame으로 번역함).[2)] 많은 연구가 죄책감과 수치심이 서로 다른 자의식 감정이라고 보고하고\\n\\n있는데, 그렇다면 라라무리 인디언들은 이 두 감정을 하나의 감정으로 인식할까? 이 논문의\\n\\n저자들은 이를 알아보기 위해 라라무리 원주민을 대상으로 평정 연구를 하였다. 먼저 이 원주\\n\\n민들에게 다양한 감정을 유발하는 상황에 관한 자료를 얻은 뒤, 네덜란드와 인도네시아의 대\\n\\n학생들에게 이 자료가 묘사하는 상황이 어떤 감정을 유발하는가 평정하도록 하였다. 그 결과\\n\\n죄책감/수치심과 관련된 18개의 상황을 설정할 수 있었고, 라라무리 인디언들이 이 상황들을\\n\\n죄책감과 수치심의 차원으로 각각 구분할 수 있는가를 평정을 통해 알아보았다. 그 결과, 비\\n\\n록 죄책감에 해당하는 단어가 존재하지 않는 라라무리 인디언들도 이 두 감정을 개념적으로\\n\\n잘 구분하는 것으로 나타났다. 물론 이 연구 결과가 감정 지각 및 경험에 언어의 역할이 없다\\n\\n고 말하지는 않는다. 하지만 적어도 죄책감이라는 단어가 없다고 해서 죄책감이라는 감정을\\n\\n수치심과 구분하지 못하는 것은 아니라는 점을 분명하게 보여주고 있다.\\n\\n지금까지 우리는 감정 지각과 감정 경험에서 언어라는 재료를 어떻게 바라봐야 하는가를 살\\n\\n펴보았다. 약한 언어결정론에서 본 바와 같이 언어는 분명 인간의 다양한 정보처리 과정에서\\n\\n상당히 중요한 영향을 미친다. 당연히 감정 경험의 전 과정 중 일정 부분에서 언어가 역할을\\n\\n한다는 것에는 어느 누구도 이의를 제기하지 않을 것이다. 감정에 관한 구성주의 견해 역시\\n\\n언어가 감정 지각을 위한 고유한 맥락을 제공하는 핵심 재료라고 주장하며 이를 지지하는 여\\n\\n러 경험 연구 결과를 보여 주었다. 하지만 우리가 이 절에서 검토한 바와 같이 감정 지각을\\n\\n2) 수치심으로 번역되는 라라무리어의 단어는 굳이 쓰자면 riwérama라고 쓸 수 있는데, 실제로는 이 언\\n\\n어는 문자가 없어서 발음상 다양한 변이가 존재한다고 한다.\\n\\n\\n-----\\n\\n위해 언어가 **어떤 역할을 하는가에 관해서는 보다 신중한 접근이 필요하다. 자칫 잘못하면 언**\\n\\n어 없이는 감정도 없다는 식의 극단적 방식으로 워프의 부활이 일어날 수 있으니 말이다.\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20363"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the text to some file in UTF8-encoding\n",
    "import pathlib\n",
    "pathlib.Path(\"output.md\").write_bytes(md_text.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\DNSOFT\\Downloads\\[AI+튜터]+초등_과학+3-2_1단원-1~2차시_수업+지도안.pdf...\n",
      "[                                        ] (0/2===================[====================                    ] (1/2)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b===================[========================================] (2/2]\n"
     ]
    }
   ],
   "source": [
    "md_text = pymupdf4llm.to_markdown(\"C:\\\\Users\\\\DNSOFT\\\\Downloads\\\\[AI+튜터]+초등_과학+3-2_1단원-1~2차시_수업+지도안.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## 과학과 교수・학습 지도안\\n\\n|Col1|과학과 교수・학|Col3|학습|지도안|Col6|Col7|Col8|Col9|Col10|\\n|---|---|---|---|---|---|---|---|---|---|\\n|단 원|1. 동물의 생활||대상|3학년 ○반 (○○명)||||||\\n||||일시|20○○.○○.○○. (○요일) ○교시|||장소||3학년 ○반 교실|\\n|학습 주제|우리 주변에는 어떤 동물이 살까요?||차시|1~2차시|||지도 교사||○ ○ ○|\\n|학습 목표|• 학교에서 볼 수 있는 동물을 관찰하고 다양한 동물이 산다는 것을 설명할 수 있다. • 주변에 사는 동물에 대한 흥미와 호기심을 갖는다.||||쪽수||과학 14~15쪽|||\\n|교수・학습 자료|교사|교과서, 교과서 음원 및 영상||||||||\\n||학생|스마트 패드||||||||\\n|지도상 유의점|• 수업 전에 학교에서 동물을 볼 수 있는 장소를 찾아 두고 그중 한곳을 선택하여 동물을 관찰하도록 한다. • 관찰 목적으로 동물을 채집할 때에는 개미나 공벌레처럼 위험하지 않은 동물로 고르고 관찰이 끝나면 살던 곳에 놓아주도록 안내한다. • 관찰할 동물이 다양하지 않을 경우 사진이나 동영상 자료를 활용한다.|||||||||\\n|단계|학습 내용|교 수・학 습 활 동||||시간||자료 및 유의점||\\n|도입|동기 유발하기 학습 문제 확인하기|▣ 생각 열기 ◦ 다양한 동물을 본 경험 이야기하기 - “우리 주변에는 어떤 동물들이 살고 있을까요?” (예시를 들어 고양이, 참새, 다람쥐 등 언급) ▣ 학습 문제 확인하기 ◦ 주변에서 볼 수 있는 동물을 관찰하고, 다양한 동물이 산다는 것을 설명할 수 있다. ◦ 주변에 사는 동물에 흥미와 호기심을 느낍니다.||||30초||◦ 스마트패드||\\n|전개|탐구 활동|[활동1] 주변에서 볼 수 있는 동물 관찰하기 ▣ 동물의 특징에 대해 생각하게 하기 ◦ 학습자의 경험과 동물의 특징 연결짓기 - “ㅇㅇ이는 나와 닮은 동물에 대해 생각해본 적 있나요?” ▣ 관찰한 동물의 이름과 특징 발표하기 ◦ 학습자가 관찰한 동물의 이름과 특징 발표하기 - “ㅇㅇ이가 동물을 하나 골라 이름과 특징을 말해 볼까요?” (예시를 들어 “개구리는 뒷다리에 물갈퀴가 있어 헤엄을 칠 수 있어요. 물과 땅을 오가며 살기도 하고요. 길고양이, 참새, 다람쥐 등 동물을 하나 골라 특징을 말해 볼까요?”)||||1분||개구리||\\n\\n\\n-----\\n\\n|마무리|식물 퀴즈 정리 및 확인|[활동2] ▣ 동물의 특징과 관련한 퀴즈 (선생님) ◦ 첫번째 문제, 이 동물은 온 몸이 털로 덮여 있어요. 4개의 다리와 꼬리를 갖고 있고요. 사람과 아주 친하고 영특한 동물이에요. 이 동물은 무엇일까요? (강아지) ◦ 두번째 문제,몸에 여러 개의 마디가 있어요. 다리는 일곱 쌍이 있습니다. 몸을 공처럼 구부릴 수 있어요. 이 동물은 무엇일까요? (공벌레) ▣ 동물의 특징과 관련한 퀴즈 (학생) ◦ 이 동물은 긴 몸을 가졌고, 비늘이 있어요. 다리는 없고, 땅을 기어다녀요.이 동물은 무엇일까요? (뱀) [활동3] 정리하기 ▣ 마무리 및 향후 단원 학습 내용 소개하기 “이번 단원에서는 다양한 동물의 특징에 대해 알아볼테니, 평소에도 관심을 갖고 주변에서 동물들을 잘 관찰해 보도록 해요.”|1분 30초 15초|강아지 공벌레 (출처:나무위키)|\\n|---|---|---|---|---|\\n\\n|※ 평가|Col2|Col3|Col4|Col5|\\n|---|---|---|---|---|\\n|성취 수준|평가 수준||평가 방법|평가 시기|\\n|◦ 여러 가지 동물을 관찰하여 특징에 따라 동물을 분류할 수 있다.|상|주변에 사는 동물의 이름과 특징을 두 가지 이상 설명하고 우리 주변에 다양한 동물이 산다는 것을 설명할 수 있다.|관찰 평가|수시|\\n||중|주변에 사는 동물의 이름과 특징을 한 가지 이상 설명하고 우리 주변에 다양한 동물이 산다는 것을 설명할 수 있다.|||\\n||하|주변에 사는 동물의 이름을 말하지 못하고 다양한 동물이 산다는 것을 설명하지 못한다.|||\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4261"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path(\"output_2.md\").write_bytes(md_text.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='## 과학과 교수・학습 지도안'),\n",
       " Document(metadata={}, page_content='|Col1|과학과 교수・학|Col3|학습|지도안|Col6|Col7|Col8|Col9|Col10|'),\n",
       " Document(metadata={}, page_content='|---|---|---|---|---|---|---|---|---|---|\\n|단 원|1. 동물의 생활||대상|3학년 ○반 (○○명)||||||'),\n",
       " Document(metadata={}, page_content='||||일시|20○○.○○.○○. (○요일) ○교시|||장소||3학년 ○반 교실|'),\n",
       " Document(metadata={}, page_content='|학습 주제|우리 주변에는 어떤 동물이 살까요?||차시|1~2차시|||지도 교사||○ ○ ○|'),\n",
       " Document(metadata={}, page_content='|학습 목표|• 학교에서 볼 수 있는 동물을 관찰하고 다양한 동물이 산다는 것을 설명할 수 있다. • 주변에 사는 동물에 대한 흥미와 호기심을'),\n",
       " Document(metadata={}, page_content='갖는다.||||쪽수||과학 14~15쪽|||'),\n",
       " Document(metadata={}, page_content='|교수・학습 자료|교사|교과서, 교과서 음원 및 영상||||||||\\n||학생|스마트 패드||||||||'),\n",
       " Document(metadata={}, page_content='|지도상 유의점|• 수업 전에 학교에서 동물을 볼 수 있는 장소를 찾아 두고 그중 한곳을 선택하여 동물을 관찰하도록 한다. • 관찰 목적으로'),\n",
       " Document(metadata={}, page_content='동물을 채집할 때에는 개미나 공벌레처럼 위험하지 않은 동물로 고르고 관찰이 끝나면 살던 곳에 놓아주도록 안내한다. • 관찰할 동물이 다양하지'),\n",
       " Document(metadata={}, page_content='않을 경우 사진이나 동영상 자료를 활용한다.|||||||||'),\n",
       " Document(metadata={}, page_content='|단계|학습 내용|교 수・학 습 활 동||||시간||자료 및 유의점||'),\n",
       " Document(metadata={}, page_content='|도입|동기 유발하기 학습 문제 확인하기|▣ 생각 열기 ◦ 다양한 동물을 본 경험 이야기하기 - “우리 주변에는 어떤 동물들이 살고'),\n",
       " Document(metadata={}, page_content='있을까요?” (예시를 들어 고양이, 참새, 다람쥐 등 언급) ▣ 학습 문제 확인하기 ◦ 주변에서 볼 수 있는 동물을 관찰하고, 다양한 동물이'),\n",
       " Document(metadata={}, page_content='산다는 것을 설명할 수 있다. ◦ 주변에 사는 동물에 흥미와 호기심을 느낍니다.||||30초||◦ 스마트패드||'),\n",
       " Document(metadata={}, page_content='|전개|탐구 활동|[활동1] 주변에서 볼 수 있는 동물 관찰하기 ▣ 동물의 특징에 대해 생각하게 하기 ◦ 학습자의 경험과 동물의 특징 연결짓기'),\n",
       " Document(metadata={}, page_content='- “ㅇㅇ이는 나와 닮은 동물에 대해 생각해본 적 있나요?” ▣ 관찰한 동물의 이름과 특징 발표하기 ◦ 학습자가 관찰한 동물의 이름과 특징'),\n",
       " Document(metadata={}, page_content='발표하기 - “ㅇㅇ이가 동물을 하나 골라 이름과 특징을 말해 볼까요?” (예시를 들어 “개구리는 뒷다리에 물갈퀴가 있어 헤엄을 칠 수 있어요.'),\n",
       " Document(metadata={}, page_content='물과 땅을 오가며 살기도 하고요. 길고양이, 참새, 다람쥐 등 동물을 하나 골라 특징을 말해 볼까요?”)||||1분||개구리||'),\n",
       " Document(metadata={}, page_content='-----'),\n",
       " Document(metadata={}, page_content='|마무리|식물 퀴즈 정리 및 확인|[활동2] ▣ 동물의 특징과 관련한 퀴즈 (선생님) ◦ 첫번째 문제, 이 동물은 온 몸이 털로 덮여 있어요.'),\n",
       " Document(metadata={}, page_content='4개의 다리와 꼬리를 갖고 있고요. 사람과 아주 친하고 영특한 동물이에요. 이 동물은 무엇일까요? (강아지) ◦ 두번째 문제,몸에 여러 개의'),\n",
       " Document(metadata={}, page_content='마디가 있어요. 다리는 일곱 쌍이 있습니다. 몸을 공처럼 구부릴 수 있어요. 이 동물은 무엇일까요? (공벌레) ▣ 동물의 특징과 관련한 퀴즈'),\n",
       " Document(metadata={}, page_content='(학생) ◦ 이 동물은 긴 몸을 가졌고, 비늘이 있어요. 다리는 없고, 땅을 기어다녀요.이 동물은 무엇일까요? (뱀) [활동3] 정리하기 ▣'),\n",
       " Document(metadata={}, page_content='마무리 및 향후 단원 학습 내용 소개하기 “이번 단원에서는 다양한 동물의 특징에 대해 알아볼테니, 평소에도 관심을 갖고 주변에서 동물들을 잘'),\n",
       " Document(metadata={}, page_content='관찰해 보도록 해요.”|1분 30초 15초|강아지 공벌레 (출처:나무위키)|'),\n",
       " Document(metadata={}, page_content='|---|---|---|---|---|'),\n",
       " Document(metadata={}, page_content='|※ 평가|Col2|Col3|Col4|Col5|\\n|---|---|---|---|---|\\n|성취 수준|평가 수준||평가 방법|평가 시기|'),\n",
       " Document(metadata={}, page_content='|◦ 여러 가지 동물을 관찰하여 특징에 따라 동물을 분류할 수 있다.|상|주변에 사는 동물의 이름과 특징을 두 가지 이상 설명하고 우리 주변에'),\n",
       " Document(metadata={}, page_content='다양한 동물이 산다는 것을 설명할 수 있다.|관찰 평가|수시|'),\n",
       " Document(metadata={}, page_content='||중|주변에 사는 동물의 이름과 특징을 한 가지 이상 설명하고 우리 주변에 다양한 동물이 산다는 것을 설명할 수 있다.|||'),\n",
       " Document(metadata={}, page_content='||하|주변에 사는 동물의 이름을 말하지 못하고 다양한 동물이 산다는 것을 설명하지 못한다.|||'),\n",
       " Document(metadata={}, page_content='-----')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "splitter = MarkdownTextSplitter(chunk_size=80, chunk_overlap=0)\n",
    "\n",
    "splitter.create_documents([md_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
