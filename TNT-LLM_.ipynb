{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://langchain-ai.github.io/langgraph/tutorials/tnt-llm/tnt-llm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TNT-LLM has three main phases:\n",
    "\n",
    "1. Generate Taxonomy\n",
    "2. Label Training Data\n",
    "3. Finetune classifier + deploy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the taxonomy, TNT-LLM proposes 5 steps:\n",
    "\n",
    "1. **Summarize** chat logs using a lower-cost LLM (batched over all logs in the sample)\n",
    "2. **Batch** the logs into random minibatches\n",
    "3. **Generate** an initial taxonomy from the first minibatch\n",
    "4. **Update** the taxonomy on each subsequent minibatch via a ritique and revise prompt\n",
    "5. **Review** the final taxonomy, scoring its quality and generating a final value using a final sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain_anthropic langsmith langchain-community\n",
    "%pip install -U sklearn langchain_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import operator\n",
    "from typing import Annotated, List, Optional\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(\"tnt-llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc(TypedDict):\n",
    "    id: str\n",
    "    content: str\n",
    "    summary: Optional[str]\n",
    "    explanation: Optional[str]\n",
    "    category: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomyGenerationState(TypedDict):\n",
    "    # The raw docs; we inject summaries within them in the first step\n",
    "    documents: List[Doc]\n",
    "    # Indices to be concise\n",
    "    minibatches: List[List[int]]\n",
    "    # Candidate Taxonomies (full trajectory)\n",
    "    clusters: Annotated[List[List[dict]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Summarize Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "summary_prompt = hub.pull(\"wfh/tnt-llm-summary-generation\").partial(\n",
    "    summary_length=20, explanation_length=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['content'], input_types={}, partial_variables={'summary_length': 20, 'explanation_length': 30}, metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'tnt-llm-summary-generation', 'lc_hub_commit_hash': 'fc880e038931afd0ff84bf0191ee75be19aa25a6595bd4d7675f8c12b23685a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['content'], input_types={}, partial_variables={}, template='# Instruction\\n\\n## Context\\n- **Goal**: You are tasked with summarizing the input text for the given use case. The summary will represent the input data for clustering in the next step.\\n- **Data**: Your input data is a conversation history between a User and an AI agent.\\n\\n# Data\\n<data>\\n{content}\\n</data>\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['explanation_length', 'summary_length'], input_types={}, partial_variables={}, template='# Questions\\n## Q1. Summarize the input text in {summary_length} words or less for the use case.\\nWrite the summary between <summary> </summary> tags.\\n\\nTips:\\n- The summary should contain the relevant information for the use case in as much detail as possible.\\n- Be concise and clear. Do not add phrases like \"This is the summary of the data ...\" or \"Summarized text: ...\".\\n- Similarly, do not reference the user (\\'the user asked XYZ\\') unless it\\'s absolutely relevant.\\n- Within {summary_length} words, include as much relevant information as possible.\\n- Do not include any line breaks in the summary.\\n- Provide your answer in **English** only.\\n\\n## Q2. Explain how you wrote the summary in {explanation_length} words or less.\\n\\n## Provide your answers between the tags <summary>your answer to Q1</summary>, <explanation>your answer to Q2</explanation>\\n\\n# Output\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary(xml_string: str) -> dict:\n",
    "    summary_pattern = r\"<summary>(.*?)</summary>\"\n",
    "    explanation_pattern = r\"<explanation>(.*?)</explanation>\"\n",
    "\n",
    "    summary_match = re.search(summary_pattern, xml_string, re.DOTALL)\n",
    "    explanation_match = re.search(explanation_pattern, xml_string, re.DOTALL)\n",
    "\n",
    "    summary = summary_match.group(1).strip() if summary_match else \"\"\n",
    "    explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
    "\n",
    "    return {\"summary\": summary, \"explanation\": explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_llm_chain = (\n",
    "    summary_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\") | StrOutputParser()\n",
    "    # Customize the tracing name for easier organization\n",
    ").with_config(run_name=\"GenerateSummary\")\n",
    "summary_chain = summary_llm_chain | parse_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine as a \"map\" operation in a map-reduce chain\n",
    "# Input: state\n",
    "# Output: state U summaries\n",
    "# Processes docs in parallel\n",
    "def get_content(state: TaxonomyGenerationState):\n",
    "    docs = state[\"documents\"]\n",
    "    return [{\"content\": doc[\"content\"]} for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_step = RunnablePassthrough.assign(\n",
    "    summaries=get_content\n",
    "    # This effectively creates a \"map\" operation\n",
    "    # Note you can make this more robust by handling individual errors\n",
    "    | RunnableLambda(func=summary_chain.batch, afunc=summary_chain.abatch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_summaries(combined: dict) -> TaxonomyGenerationState:\n",
    "    summaries = combined[\"summaries\"]\n",
    "    documents = combined[\"documents\"]\n",
    "    return {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"id\": doc[\"id\"],\n",
    "                \"content\": doc[\"content\"],\n",
    "                \"summary\": summ_info[\"summary\"],\n",
    "                \"explanation\": summ_info[\"explanation\"],\n",
    "            }\n",
    "            for doc, summ_info in zip(documents, summaries)\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is actually the node itself!\n",
    "map_reduce_chain = map_step | reduce_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split into Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_minibatches(state: TaxonomyGenerationState, config: RunnableConfig):\n",
    "    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n",
    "    original = state[\"documents\"]\n",
    "    indices = list(range(len(original)))\n",
    "    random.shuffle(indices)\n",
    "    if len(indices) < batch_size:\n",
    "        # Don't pad needlessly if we can't fill a single batch\n",
    "        return [indices]\n",
    "\n",
    "    num_full_batches = len(indices) // batch_size\n",
    "\n",
    "    batches = [\n",
    "        indices[i * batch_size : (i + 1) * batch_size] for i in range(num_full_batches)\n",
    "    ]\n",
    "\n",
    "    leftovers = len(indices) % batch_size\n",
    "    if leftovers:\n",
    "        last_batch = indices[num_full_batches * batch_size :]\n",
    "        elements_to_add = batch_size - leftovers\n",
    "        last_batch += random.sample(indices, elements_to_add)\n",
    "        batches.append(last_batch)\n",
    "\n",
    "    return {\n",
    "        \"minibatches\": batches,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Taxonomy Generation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "\n",
    "def parse_taxa(output_text: str) -> Dict:\n",
    "    \"\"\"Extract the taxonomy from the generated output.\"\"\"\n",
    "    cluster_matches = re.findall(\n",
    "        r\"\\s*<id>(.*?)</id>\\s*<name>(.*?)</name>\\s*<description>(.*?)</description>\\s*\",\n",
    "        output_text,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "    clusters = [\n",
    "        {\"id\": id.strip(), \"name\": name.strip(), \"description\": description.strip()}\n",
    "        for id, name, description in cluster_matches\n",
    "    ]\n",
    "    # We don't parse the explanation since it isn't used downstream\n",
    "    return {\"clusters\": clusters}\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Doc]) -> str:\n",
    "    xml_table = \"<conversations>\\n\"\n",
    "    for doc in docs:\n",
    "        xml_table += f'<conv_summ id={doc[\"id\"]}>{doc[\"summary\"]}</conv_summ>\\n'\n",
    "    xml_table += \"</conversations>\"\n",
    "    return xml_table\n",
    "\n",
    "\n",
    "def format_taxonomy(clusters):\n",
    "    xml = \"<cluster_table>\\n\"\n",
    "    for label in clusters:\n",
    "        xml += \"  <cluster>\\n\"\n",
    "        xml += f'    <id>{label[\"id\"]}</id>\\n'\n",
    "        xml += f'    <name>{label[\"name\"]}</name>\\n'\n",
    "        xml += f'    <description>{label[\"description\"]}</description>\\n'\n",
    "        xml += \"  </cluster>\\n\"\n",
    "    xml += \"</cluster_table>\"\n",
    "    return xml\n",
    "\n",
    "\n",
    "def invoke_taxonomy_chain(\n",
    "    chain: Runnable,\n",
    "    state: TaxonomyGenerationState,\n",
    "    config: RunnableConfig,\n",
    "    mb_indices: List[int],\n",
    ") -> TaxonomyGenerationState:\n",
    "    configurable = config[\"configurable\"]\n",
    "    docs = state[\"documents\"]\n",
    "    minibatch = [docs[idx] for idx in mb_indices]\n",
    "    data_table_xml = format_docs(minibatch)\n",
    "\n",
    "    previous_taxonomy = state[\"clusters\"][-1] if state[\"clusters\"] else []\n",
    "    cluster_table_xml = format_taxonomy(previous_taxonomy)\n",
    "\n",
    "    updated_taxonomy = chain.invoke(\n",
    "        {\n",
    "            \"data_xml\": data_table_xml,\n",
    "            \"use_case\": configurable[\"use_case\"],\n",
    "            \"cluster_table_xml\": cluster_table_xml,\n",
    "            \"suggestion_length\": configurable.get(\"suggestion_length\", 30),\n",
    "            \"cluster_name_length\": configurable.get(\"cluster_name_length\", 10),\n",
    "            \"cluster_description_length\": configurable.get(\n",
    "                \"cluster_description_length\", 30\n",
    "            ),\n",
    "            \"explanation_length\": configurable.get(\"explanation_length\", 20),\n",
    "            \"max_num_clusters\": configurable.get(\"max_num_clusters\", 25),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"clusters\": [updated_taxonomy[\"clusters\"]],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate initial taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# We will share an LLM for each step of the generate -> update -> review cycle\n",
    "# You may want to consider using Opus or another more powerful model for this\n",
    "taxonomy_generation_llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", max_tokens_to_sample=2000\n",
    ")\n",
    "\n",
    "\n",
    "## Initial generation\n",
    "taxonomy_generation_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-generation\").partial(\n",
    "    use_case=\"Generate the taxonomy that can be used to label the user intent in the conversation.\",\n",
    ")\n",
    "\n",
    "taxa_gen_llm_chain = (\n",
    "    taxonomy_generation_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name=\"GenerateTaxonomy\")\n",
    "\n",
    "\n",
    "generate_taxonomy_chain = taxa_gen_llm_chain | parse_taxa\n",
    "\n",
    "\n",
    "def generate_taxonomy(\n",
    "    state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    return invoke_taxonomy_chain(\n",
    "        generate_taxonomy_chain, state, config, state[\"minibatches\"][0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Update Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "taxonomy_update_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-update\")\n",
    "\n",
    "taxa_update_llm_chain = (\n",
    "    taxonomy_update_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name=\"UpdateTaxonomy\")\n",
    "\n",
    "\n",
    "update_taxonomy_chain = taxa_update_llm_chain | parse_taxa\n",
    "\n",
    "\n",
    "def update_taxonomy(\n",
    "    state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    which_mb = len(state[\"clusters\"]) % len(state[\"minibatches\"])\n",
    "    return invoke_taxonomy_chain(\n",
    "        update_taxonomy_chain, state, config, state[\"minibatches\"][which_mb]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Review Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNSOFT\\test_notebooks_\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "taxonomy_review_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-review\")\n",
    "\n",
    "taxa_review_llm_chain = (\n",
    "    taxonomy_review_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name=\"ReviewTaxonomy\")\n",
    "\n",
    "\n",
    "review_taxonomy_chain = taxa_review_llm_chain | parse_taxa\n",
    "\n",
    "\n",
    "def review_taxonomy(\n",
    "    state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n",
    "    original = state[\"documents\"]\n",
    "    indices = list(range(len(original)))\n",
    "    random.shuffle(indices)\n",
    "    return invoke_taxonomy_chain(\n",
    "        review_taxonomy_chain, state, config, indices[:batch_size]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(TaxonomyGenerationState)\n",
    "graph.add_node(\"summarize\", map_reduce_chain)\n",
    "graph.add_node(\"get_minibatches\", get_minibatches)\n",
    "graph.add_node(\"generate_taxonomy\", generate_taxonomy)\n",
    "graph.add_node(\"update_taxonomy\", update_taxonomy)\n",
    "graph.add_node(\"review_taxonomy\", review_taxonomy)\n",
    "\n",
    "graph.add_edge(\"summarize\", \"get_minibatches\")\n",
    "graph.add_edge(\"get_minibatches\", \"generate_taxonomy\")\n",
    "graph.add_edge(\"generate_taxonomy\", \"update_taxonomy\")\n",
    "\n",
    "\n",
    "def should_review(state: TaxonomyGenerationState) -> str:\n",
    "    num_minibatches = len(state[\"minibatches\"])\n",
    "    num_revisions = len(state[\"clusters\"])\n",
    "    if num_revisions < num_minibatches:\n",
    "        return \"update_taxonomy\"\n",
    "    return \"review_taxonomy\"\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"update_taxonomy\",\n",
    "    should_review,\n",
    "    # Optional (but required for the diagram to be drawn correctly below)\n",
    "    {\"update_taxonomy\": \"update_taxonomy\", \"review_taxonomy\": \"review_taxonomy\"},\n",
    ")\n",
    "graph.add_edge(\"review_taxonomy\", END)\n",
    "\n",
    "graph.add_edge(START, \"summarize\")\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
