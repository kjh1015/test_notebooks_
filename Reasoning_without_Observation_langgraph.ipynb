{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community langchain_openai langchain_fireworks langgraph wikipedia duckduckgo-search tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDG\n",
    "# search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "search_engine = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = DuckDuckGoSearchAPIWrapper()._ddgs_text(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Australian Open men\\'s final 2024: Jannik Sinner beats Daniil Medvedev in Melbourne final. Published. 28 January 2024. ... Sinner clinched victory with a forehand winner down the line, falling to ... The world No.4 clinched the Australian Open 2024 men\\'s singles title in stunning fashion, overcoming third seed Daniil Medvedev 3-6 3-6 6-4 6-4 6-3 in three hours and 44 minutes on Sunday night at Rod Laver Arena. MORE: All the stats from the match Jannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men\\'s singles final, earning him his first ever Grand Slam title.The 22-year-old ... Australian Open 2024 took place from 14-28 Jan 2024 Jannik Sinner beat Daniil Medvedev check the winners list of Men & Women Singles, Doubles, & Mixed Doubles. More from the 2024 Australian Open winner Jannik Sinner: \"I want to thank all the sponsors, ball kids, chair umpire, everyone who makes this event so good. It\\'s the happiest Slam, it is a very ...'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DuckDuckGoSearchAPIWrapper().run(\"the winner of the 2024 Men's Australian Open\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Alcoholics Anonymous - Wikipedia',\n",
       "  'href': 'https://en.wikipedia.org/wiki/Alcoholics_Anonymous',\n",
       "  'body': 'Alcoholics Anonymous (AA) is a global, peer-led mutual-aid fellowship focused on an abstinence-based recovery model from alcoholism through its spiritually-inclined twelve-step program. [1] The organization adheres to Twelve Traditions that emphasize anonymity, the absence of a hierarchical structure, and principles of being free to all, non-promotional, non-professional, unaffiliated, non ...'},\n",
       " {'title': 'American Airlines maintains AAdvantage status and reward levels for ...',\n",
       "  'href': 'https://news.aa.com/news/news-details/2024/American-Airlines-maintains-AAdvantage-status-and-reward-levels-for-second-year-in-a-row-AADV-12/default.aspx',\n",
       "  'body': \"American Airlines continues to make the AAdvantage® program more rewarding for its members as it updates the industry's longest-running loyalty program for 2025. For the second year in a row, the requirements for status and reward levels will remain the same, making it straightforward to achieve or retain status and all its benefits.\"},\n",
       " {'title': 'Customer service plan − Support − American Airlines',\n",
       "  'href': 'https://www.aa.com/i18n/customer-service/support/customer-service-plan.jsp?os=qtfT_2',\n",
       "  'body': 'Anyone wishing to enroll in the AAdvantage ® program can do so instantly at any American Airlines or American Eagle ticket counter, online at the aa.com website, or by calling 800-433-7300. AAdvantage ® members earn mileage credits by flying on American Airlines, American Eagle and one world ® carriers, as well as on other airline participants.'},\n",
       " {'title': 'Travel alerts − Travel information − American ... - American Airlines',\n",
       "  'href': 'https://www.aa.com/i18n/travel-info/travel-alerts.jsp?anchorLocation=DirectURL&title=travelalerts&os=ios0',\n",
       "  'body': 'AA.com® Link opens in new window. Site may not meet accessibility guidelines. AA.com® Receipts and refunds Bag and optional fees Customer service and contingency plans Conditions of carriage FAQs Contact Legal, privacy, copyright ...'},\n",
       " {'title': 'American Airlines - Wikipedia',\n",
       "  'href': 'https://en.wikipedia.org/wiki/American_Airlines',\n",
       "  'body': 'American Airlines [8] is a major airline in the United States headquartered in Fort Worth, Texas, within the Dallas-Fort Worth metroplex.It is the largest airline in the world when measured by scheduled passengers carried, revenue passenger mile, and daily flights. American, along with its regional subsidiaries and contractors operating under the brand name American Eagle, operate an ...'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class ReWOO(TypedDict):\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: dict\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"For the following task, make plans that can solve the problem step by step. For each plan, indicate \\\n",
    "which external tool together with tool input to retrieve evidence. You can store the evidence into a \\\n",
    "variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
    "\n",
    "Tools can be one of the following:\n",
    "(1) Duckduckgo[input]: Worker that searches results from Duckduckgo. Useful when you need to find short\n",
    "and succinct answers about a specific topic. The input should be a search query.\n",
    "(2) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general\n",
    "world knowledge and common sense. Prioritize it when you are confident in solving the problem\n",
    "yourself. Input can be any instruction.\n",
    "\n",
    "For example,\n",
    "Task: Thomas, Toby, and Rebecca worked a total of 157 hours in one week. Thomas worked x\n",
    "hours. Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours\n",
    "less than Toby. How many hours did Rebecca work?\n",
    "Plan: Given Thomas worked x hours, translate the problem into algebraic expressions and solve\n",
    "with Wolfram Alpha. #E1 = WolframAlpha[Solve x + (2x − 10) + ((2x − 10) − 8) = 157]\n",
    "Plan: Find out the number of hours Thomas worked. #E2 = LLM[What is x, given #E1]\n",
    "Plan: Calculate the number of hours Rebecca worked. #E3 = Calculator[(2 ∗ #E2 − 10) − 8]\n",
    "\n",
    "Begin! \n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\n",
    "Task: {task}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"what is the exact hometown of the 2024 mens australian open winner\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"How to automatically optimize prompts for LLMs?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(prompt.format(task=task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the following task, make plans that can solve the problem step by step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\\n\\nTools can be one of the following:\\n(1) Duckduckgo[input]: Worker that searches results from Duckduckgo. Useful when you need to find short\\nand succinct answers about a specific topic. The input should be a search query.\\n(2) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general\\nworld knowledge and common sense. Prioritize it when you are confident in solving the problem\\nyourself. Input can be any instruction.\\n\\nFor example,\\nTask: Thomas, Toby, and Rebecca worked a total of 157 hours in one week. Thomas worked x\\nhours. Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours\\nless than Toby. How many hours did Rebecca work?\\nPlan: Given Thomas worked x hours, translate the problem into algebraic expressions and solve\\nwith Wolfram Alpha. #E1 = WolframAlpha[Solve x + (2x − 10) + ((2x − 10) − 8) = 157]\\nPlan: Find out the number of hours Thomas worked. #E2 = LLM[What is x, given #E1]\\nPlan: Calculate the number of hours Rebecca worked. #E3 = Calculator[(2 ∗ #E2 − 10) − 8]\\n\\nBegin! \\nDescribe your plans with rich details. Each Plan should be followed by only one #E.\\n\\nTask: How to automatically optimize prompts for LLMs?'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(task=task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a step-by-step plan to automatically optimize prompts for LLMs:\n",
      "\n",
      "Plan: Research current methods and techniques for prompt optimization. This will provide a foundation for understanding the state-of-the-art approaches.\n",
      "#E1 = Duckduckgo[automatic prompt optimization for large language models]\n",
      "\n",
      "Plan: Analyze the search results to identify key strategies and tools used for automatic prompt optimization.\n",
      "#E2 = LLM[Summarize the main approaches and tools for automatic prompt optimization based on the information in #E1]\n",
      "\n",
      "Plan: Investigate specific algorithms or frameworks mentioned in the search results that are commonly used for prompt optimization.\n",
      "#E3 = Duckduckgo[prompt optimization algorithms for LLMs]\n",
      "\n",
      "Plan: Outline the general steps involved in an automatic prompt optimization process based on the information gathered.\n",
      "#E4 = LLM[Based on #E1, #E2, and #E3, provide a step-by-step outline of a general automatic prompt optimization process for LLMs]\n",
      "\n",
      "Plan: Research evaluation metrics used to assess the quality of optimized prompts.\n",
      "#E5 = Duckduckgo[evaluation metrics for LLM prompt optimization]\n",
      "\n",
      "Plan: Summarize the key evaluation metrics and their importance in the optimization process.\n",
      "#E6 = LLM[Summarize the main evaluation metrics used in LLM prompt optimization and explain their significance based on the information in #E5]\n",
      "\n",
      "Plan: Investigate potential challenges and limitations of automatic prompt optimization techniques.\n",
      "#E7 = LLM[Based on the information gathered in #E1, #E2, #E3, #E4, #E5, and #E6, what are the main challenges and limitations of automatic prompt optimization for LLMs?]\n",
      "\n",
      "Plan: Propose potential solutions or areas for improvement in automatic prompt optimization techniques.\n",
      "#E8 = LLM[Given the challenges and limitations identified in #E7, suggest potential solutions or areas for improvement in automatic prompt optimization for LLMs]\n",
      "\n",
      "Plan: Compile a comprehensive guide on how to automatically optimize prompts for LLMs, incorporating all the information gathered.\n",
      "#E9 = LLM[Create a comprehensive guide on how to automatically optimize prompts for LLMs, incorporating the information from #E1 through #E8. Include steps, tools, algorithms, evaluation metrics, challenges, and potential improvements.]\n"
     ]
    }
   ],
   "source": [
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to match expressions of the form E#... = ...[...]\n",
    "regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", prompt)])\n",
    "planner = prompt_template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plan(state: ReWOO): # state by a TypedDict\n",
    "    task = state[\"task\"]\n",
    "    result = planner.invoke({\"task\": task})\n",
    "    # Find all matches in the sample text\n",
    "    matches = re.findall(regex_pattern, result.content)\n",
    "    return {\"steps\": matches, \"plan_string\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDG\n",
    "# search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_current_task(state: ReWOO):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDuckGoSearchAPIWrapper()._ddgs_text(\"aa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_execution(state: ReWOO):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _step = _get_current_task(state)\n",
    "    _, step_name, tool, tool_input = state[\"steps\"][_step - 1]\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    \n",
    "    # Replace any result placeholders in the tool_input\n",
    "    for k, v in _results.items():\n",
    "        tool_input = tool_input.replace(k, v)\n",
    "    \n",
    "    if tool == \"Duckduckgo\":\n",
    "        # Extract just the actual search query\n",
    "        if isinstance(tool_input, str):\n",
    "            # If the input contains article metadata, extract just the last part after the last comma\n",
    "            if \"snippet:\" in tool_input:\n",
    "                tool_input = tool_input.split(\",\")[-1].strip()\n",
    "            # Remove any metadata markers\n",
    "            tool_input = tool_input.replace(\"snippet:\", \"\").replace(\"title:\", \"\").replace(\"link:\", \"\")\n",
    "            # Clean up any remaining special characters and extra whitespace\n",
    "            tool_input = \" \".join(tool_input.split())\n",
    "            \n",
    "        result = search.invoke(tool_input)\n",
    "    elif tool == \"LLM\":\n",
    "        result = model.invoke(tool_input)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    _results[step_name] = str(result)\n",
    "    return {\"results\": _results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_prompt = \"\"\"Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \\\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \\\n",
    "contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. Respond with the answer\n",
    "directly with no extra words.\n",
    "\n",
    "Task: {task}\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ReWOO):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, tool, tool_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            tool_input = tool_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {tool}[{tool_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = model.invoke(prompt)\n",
    "    return {\"result\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        # We have executed all tasks\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        # We are still executing tasks, loop back to the \"tool\" node\n",
    "        return \"tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "graph = StateGraph(ReWOO)\n",
    "graph.add_node(\"plan\", get_plan)\n",
    "graph.add_node(\"tool\", tool_execution)\n",
    "graph.add_node(\"solve\", solve)\n",
    "graph.add_edge(\"plan\", \"tool\")\n",
    "graph.add_edge(\"solve\", END)\n",
    "graph.add_conditional_edges(\"tool\", _route)\n",
    "graph.add_edge(START, \"plan\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': {'steps': [(\"First, let's search for information about prompt optimization techniques for LLMs to get an overview of current approaches.\", '#E1', 'Duckduckgo', 'automatic prompt optimization for large language models'), (\"Now that we have some general information, let's focus on identifying the key components of an automatic prompt optimization system.\", '#E2', 'LLM', 'Based on the information in #E1, what are the main components or steps involved in automatic prompt optimization for LLMs?'), (\"Let's explore specific algorithms or methods used for prompt optimization.\", '#E3', 'Duckduckgo', 'prompt optimization algorithms for LLMs'), ('We should investigate tools or frameworks that are commonly used for automatic prompt optimization.', '#E4', 'Duckduckgo', 'tools and frameworks for automatic prompt optimization LLMs'), (\"Now, let's synthesize the information we've gathered to create a comprehensive guide on how to automatically optimize prompts for LLMs.\", '#E5', 'LLM', 'Using the information from #E1, #E2, #E3, and #E4, provide a detailed step-by-step guide on how to automatically optimize prompts for LLMs. Include specific techniques, algorithms, and tools where applicable.'), (\"To ensure our guide is up-to-date and practical, let's search for any recent advancements or case studies in automatic prompt optimization.\", '#E6', 'Duckduckgo', 'recent advancements in automatic prompt optimization for LLMs case studies'), (\"Finally, let's refine our guide by incorporating the latest advancements and practical examples from real-world applications.\", '#E7', 'LLM', 'Update the guide from #E5 with relevant information from #E6, focusing on recent advancements and practical examples of automatic prompt optimization for LLMs.')], 'plan_string': \"Here's a step-by-step plan to automatically optimize prompts for LLMs:\\n\\nPlan: First, let's search for information about prompt optimization techniques for LLMs to get an overview of current approaches.\\n#E1 = Duckduckgo[automatic prompt optimization for large language models]\\n\\nPlan: Now that we have some general information, let's focus on identifying the key components of an automatic prompt optimization system.\\n#E2 = LLM[Based on the information in #E1, what are the main components or steps involved in automatic prompt optimization for LLMs?]\\n\\nPlan: Let's explore specific algorithms or methods used for prompt optimization.\\n#E3 = Duckduckgo[prompt optimization algorithms for LLMs]\\n\\nPlan: We should investigate tools or frameworks that are commonly used for automatic prompt optimization.\\n#E4 = Duckduckgo[tools and frameworks for automatic prompt optimization LLMs]\\n\\nPlan: Now, let's synthesize the information we've gathered to create a comprehensive guide on how to automatically optimize prompts for LLMs.\\n#E5 = LLM[Using the information from #E1, #E2, #E3, and #E4, provide a detailed step-by-step guide on how to automatically optimize prompts for LLMs. Include specific techniques, algorithms, and tools where applicable.]\\n\\nPlan: To ensure our guide is up-to-date and practical, let's search for any recent advancements or case studies in automatic prompt optimization.\\n#E6 = Duckduckgo[recent advancements in automatic prompt optimization for LLMs case studies]\\n\\nPlan: Finally, let's refine our guide by incorporating the latest advancements and practical examples from real-world applications.\\n#E7 = LLM[Update the guide from #E5 with relevant information from #E6, focusing on recent advancements and practical examples of automatic prompt optimization for LLMs.]\"}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer', '#E2': 'content=\"Based on the information provided in the snippets, the main components or steps involved in automatic prompt optimization for Large Language Models (LLMs) typically include:\\\\n\\\\n1. Input Data Preparation: The process starts with preparing input data, which may include both clean and potentially perturbed data to ensure robustness.\\\\n\\\\n2. Prompt Generation: Initial prompts are generated, either manually or through automated means, as a starting point for optimization.\\\\n\\\\n3. Iterative Optimization: The prompts undergo an iterative process of refinement and improvement. This may involve:\\\\n   - Testing prompts against the model\\\\n   - Evaluating performance based on predefined metrics\\\\n   - Adjusting prompts based on results\\\\n\\\\n4. Performance Evaluation: The quality of prompts is assessed based on the semantic and structural integrity of the output, as well as the model\\'s performance on various tasks.\\\\n\\\\n5. Robustness Testing: As mentioned in the BATprompt snippet, testing prompts against perturbed inputs to ensure consistent performance across various input conditions.\\\\n\\\\n6. Human Oversight: Despite automation, human prompt engineers still play a crucial role in overseeing the process and making high-level decisions.\\\\n\\\\n7. Integration with AI Platforms: As seen in the Vertex AI Prompt Optimizer example, these techniques can be integrated into larger AI platforms for easier access and use.\\\\n\\\\n8. Adaptation to Specific Models: The optimization process may need to be tailored to the specific LLM being used, as different models may respond differently to prompts.\\\\n\\\\n9. Incorporation of Research Findings: The process is continually evolving based on new research, such as the APO methods mentioned in the Google Cloud snippet.\\\\n\\\\nIt\\'s important to note that while these systems aim to automate much of the prompt optimization process, they are generally designed to assist rather than replace human prompt engineers, serving as a tool to enhance efficiency and performance in prompt development for LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01Nxc84zUqNRxA1XNGs1F8NX\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 519, \\'output_tokens\\': 419}} id=\\'run-faeebf31-6288-4b24-af12-68646560c7b0-0\\' usage_metadata={\\'input_tokens\\': 519, \\'output_tokens\\': 419, \\'total_tokens\\': 938, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer', '#E2': 'content=\"Based on the information provided in the snippets, the main components or steps involved in automatic prompt optimization for Large Language Models (LLMs) typically include:\\\\n\\\\n1. Input Data Preparation: The process starts with preparing input data, which may include both clean and potentially perturbed data to ensure robustness.\\\\n\\\\n2. Prompt Generation: Initial prompts are generated, either manually or through automated means, as a starting point for optimization.\\\\n\\\\n3. Iterative Optimization: The prompts undergo an iterative process of refinement and improvement. This may involve:\\\\n   - Testing prompts against the model\\\\n   - Evaluating performance based on predefined metrics\\\\n   - Adjusting prompts based on results\\\\n\\\\n4. Performance Evaluation: The quality of prompts is assessed based on the semantic and structural integrity of the output, as well as the model\\'s performance on various tasks.\\\\n\\\\n5. Robustness Testing: As mentioned in the BATprompt snippet, testing prompts against perturbed inputs to ensure consistent performance across various input conditions.\\\\n\\\\n6. Human Oversight: Despite automation, human prompt engineers still play a crucial role in overseeing the process and making high-level decisions.\\\\n\\\\n7. Integration with AI Platforms: As seen in the Vertex AI Prompt Optimizer example, these techniques can be integrated into larger AI platforms for easier access and use.\\\\n\\\\n8. Adaptation to Specific Models: The optimization process may need to be tailored to the specific LLM being used, as different models may respond differently to prompts.\\\\n\\\\n9. Incorporation of Research Findings: The process is continually evolving based on new research, such as the APO methods mentioned in the Google Cloud snippet.\\\\n\\\\nIt\\'s important to note that while these systems aim to automate much of the prompt optimization process, they are generally designed to assist rather than replace human prompt engineers, serving as a tool to enhance efficiency and performance in prompt development for LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01Nxc84zUqNRxA1XNGs1F8NX\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 519, \\'output_tokens\\': 419}} id=\\'run-faeebf31-6288-4b24-af12-68646560c7b0-0\\' usage_metadata={\\'input_tokens\\': 519, \\'output_tokens\\': 419, \\'total_tokens\\': 938, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E3': 'snippet: LLMs are good prompt engineers. ... While other prompt optimization algorithms commonly see diminishing returns and a saturation in performance when extending the optimization process to larger numbers of iterations [1], Promptbreeder dynamically adapts the optimization process over time, allowing intricate prompts to be discovered that are ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: This process is illustrated in Figure 1, which shows a RAG prompt designed to translate user queries into a domain-specific language (DSL), also known as semantic parsing. Figure 1: A RAG prompt is used for a semantic parsing task. The underlying prompt consists of three larger parts, each with a variety of aspects that can be optimized., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: The performance of Large Language Models (LLMs) in a specific task is critically dependent upon the instruction (prompt) received. Prompt optimization aims at selecting a sequence of tokens which concatenated with the text query yields an instruction which optimizes..., title: A Bayesian Approach for Prompt Optimization in LLMs, link: https://link.springer.com/chapter/10.1007/978-3-031-75623-8_28, snippet: 2 Opro: Llm as the Optimizer and 2.1 Desirables of Optimization by Llms. 2.2 Meta-Prompt Design. 3 Motivating Example: Mathematical Optimization and 3.1 Linear Regression. 3.2 Traveling Salesman Problem (TSP) ... OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human ..., title: Optimizing Prompts with LLMs: Key Findings and Future Directions, link: https://hackernoon.com/optimizing-prompts-with-llms-key-findings-and-future-directions'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer', '#E2': 'content=\"Based on the information provided in the snippets, the main components or steps involved in automatic prompt optimization for Large Language Models (LLMs) typically include:\\\\n\\\\n1. Input Data Preparation: The process starts with preparing input data, which may include both clean and potentially perturbed data to ensure robustness.\\\\n\\\\n2. Prompt Generation: Initial prompts are generated, either manually or through automated means, as a starting point for optimization.\\\\n\\\\n3. Iterative Optimization: The prompts undergo an iterative process of refinement and improvement. This may involve:\\\\n   - Testing prompts against the model\\\\n   - Evaluating performance based on predefined metrics\\\\n   - Adjusting prompts based on results\\\\n\\\\n4. Performance Evaluation: The quality of prompts is assessed based on the semantic and structural integrity of the output, as well as the model\\'s performance on various tasks.\\\\n\\\\n5. Robustness Testing: As mentioned in the BATprompt snippet, testing prompts against perturbed inputs to ensure consistent performance across various input conditions.\\\\n\\\\n6. Human Oversight: Despite automation, human prompt engineers still play a crucial role in overseeing the process and making high-level decisions.\\\\n\\\\n7. Integration with AI Platforms: As seen in the Vertex AI Prompt Optimizer example, these techniques can be integrated into larger AI platforms for easier access and use.\\\\n\\\\n8. Adaptation to Specific Models: The optimization process may need to be tailored to the specific LLM being used, as different models may respond differently to prompts.\\\\n\\\\n9. Incorporation of Research Findings: The process is continually evolving based on new research, such as the APO methods mentioned in the Google Cloud snippet.\\\\n\\\\nIt\\'s important to note that while these systems aim to automate much of the prompt optimization process, they are generally designed to assist rather than replace human prompt engineers, serving as a tool to enhance efficiency and performance in prompt development for LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01Nxc84zUqNRxA1XNGs1F8NX\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 519, \\'output_tokens\\': 419}} id=\\'run-faeebf31-6288-4b24-af12-68646560c7b0-0\\' usage_metadata={\\'input_tokens\\': 519, \\'output_tokens\\': 419, \\'total_tokens\\': 938, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E3': 'snippet: LLMs are good prompt engineers. ... While other prompt optimization algorithms commonly see diminishing returns and a saturation in performance when extending the optimization process to larger numbers of iterations [1], Promptbreeder dynamically adapts the optimization process over time, allowing intricate prompts to be discovered that are ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: This process is illustrated in Figure 1, which shows a RAG prompt designed to translate user queries into a domain-specific language (DSL), also known as semantic parsing. Figure 1: A RAG prompt is used for a semantic parsing task. The underlying prompt consists of three larger parts, each with a variety of aspects that can be optimized., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: The performance of Large Language Models (LLMs) in a specific task is critically dependent upon the instruction (prompt) received. Prompt optimization aims at selecting a sequence of tokens which concatenated with the text query yields an instruction which optimizes..., title: A Bayesian Approach for Prompt Optimization in LLMs, link: https://link.springer.com/chapter/10.1007/978-3-031-75623-8_28, snippet: 2 Opro: Llm as the Optimizer and 2.1 Desirables of Optimization by Llms. 2.2 Meta-Prompt Design. 3 Motivating Example: Mathematical Optimization and 3.1 Linear Regression. 3.2 Traveling Salesman Problem (TSP) ... OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human ..., title: Optimizing Prompts with LLMs: Key Findings and Future Directions, link: https://hackernoon.com/optimizing-prompts-with-llms-key-findings-and-future-directions', '#E4': \"snippet: To align with previous research, we used eight zero-shot BigBench classification tasks where the baseline prompt for GPT-3.5 achieved an accuracy of less than 0.9. We compared it against Automatic Prompt Optimization (APO) and GrIPS, applying open-source models Mixtral 7x8B and Llama-2 70B, alongside GPT-3.5 as backend LLMs., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving ..., title: PromptWizard: Task-Aware Prompt Optimization Framework, link: https://arxiv.org/abs/2405.18369, snippet: Enter Microsoft's Automatic Prompt Optimization (APO) framework. Introduced in a recent paper from Microsoft AI Research [2], APO represents a significant step towards automating the prompt engineering process in a generalizable and efficient way. ... APO promises to greatly expand the accessibility and impact of these cutting-edge tools. How ..., title: Microsoft's Automatic Prompt Optimization Framework: A Leap Forward for ..., link: https://www.33rdsquare.com/microsoft-introduces-automatic-prompt-optimization-framework-for-llms/, snippet: PromptWizard (PW) is designed to automate and simplify prompt optimization. It combines iterative feedback from LLMs with efficient exploration and refinement techniques to create highly effective prompts within minutes. PromptWizard optimizes both the instruction and the in-context learning examples., title: PromptWizard: The future of prompt optimization through feedback-driven ..., link: https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/\"}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer', '#E2': 'content=\"Based on the information provided in the snippets, the main components or steps involved in automatic prompt optimization for Large Language Models (LLMs) typically include:\\\\n\\\\n1. Input Data Preparation: The process starts with preparing input data, which may include both clean and potentially perturbed data to ensure robustness.\\\\n\\\\n2. Prompt Generation: Initial prompts are generated, either manually or through automated means, as a starting point for optimization.\\\\n\\\\n3. Iterative Optimization: The prompts undergo an iterative process of refinement and improvement. This may involve:\\\\n   - Testing prompts against the model\\\\n   - Evaluating performance based on predefined metrics\\\\n   - Adjusting prompts based on results\\\\n\\\\n4. Performance Evaluation: The quality of prompts is assessed based on the semantic and structural integrity of the output, as well as the model\\'s performance on various tasks.\\\\n\\\\n5. Robustness Testing: As mentioned in the BATprompt snippet, testing prompts against perturbed inputs to ensure consistent performance across various input conditions.\\\\n\\\\n6. Human Oversight: Despite automation, human prompt engineers still play a crucial role in overseeing the process and making high-level decisions.\\\\n\\\\n7. Integration with AI Platforms: As seen in the Vertex AI Prompt Optimizer example, these techniques can be integrated into larger AI platforms for easier access and use.\\\\n\\\\n8. Adaptation to Specific Models: The optimization process may need to be tailored to the specific LLM being used, as different models may respond differently to prompts.\\\\n\\\\n9. Incorporation of Research Findings: The process is continually evolving based on new research, such as the APO methods mentioned in the Google Cloud snippet.\\\\n\\\\nIt\\'s important to note that while these systems aim to automate much of the prompt optimization process, they are generally designed to assist rather than replace human prompt engineers, serving as a tool to enhance efficiency and performance in prompt development for LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01Nxc84zUqNRxA1XNGs1F8NX\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 519, \\'output_tokens\\': 419}} id=\\'run-faeebf31-6288-4b24-af12-68646560c7b0-0\\' usage_metadata={\\'input_tokens\\': 519, \\'output_tokens\\': 419, \\'total_tokens\\': 938, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E3': 'snippet: LLMs are good prompt engineers. ... While other prompt optimization algorithms commonly see diminishing returns and a saturation in performance when extending the optimization process to larger numbers of iterations [1], Promptbreeder dynamically adapts the optimization process over time, allowing intricate prompts to be discovered that are ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: This process is illustrated in Figure 1, which shows a RAG prompt designed to translate user queries into a domain-specific language (DSL), also known as semantic parsing. Figure 1: A RAG prompt is used for a semantic parsing task. The underlying prompt consists of three larger parts, each with a variety of aspects that can be optimized., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: The performance of Large Language Models (LLMs) in a specific task is critically dependent upon the instruction (prompt) received. Prompt optimization aims at selecting a sequence of tokens which concatenated with the text query yields an instruction which optimizes..., title: A Bayesian Approach for Prompt Optimization in LLMs, link: https://link.springer.com/chapter/10.1007/978-3-031-75623-8_28, snippet: 2 Opro: Llm as the Optimizer and 2.1 Desirables of Optimization by Llms. 2.2 Meta-Prompt Design. 3 Motivating Example: Mathematical Optimization and 3.1 Linear Regression. 3.2 Traveling Salesman Problem (TSP) ... OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human ..., title: Optimizing Prompts with LLMs: Key Findings and Future Directions, link: https://hackernoon.com/optimizing-prompts-with-llms-key-findings-and-future-directions', '#E4': \"snippet: To align with previous research, we used eight zero-shot BigBench classification tasks where the baseline prompt for GPT-3.5 achieved an accuracy of less than 0.9. We compared it against Automatic Prompt Optimization (APO) and GrIPS, applying open-source models Mixtral 7x8B and Llama-2 70B, alongside GPT-3.5 as backend LLMs., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving ..., title: PromptWizard: Task-Aware Prompt Optimization Framework, link: https://arxiv.org/abs/2405.18369, snippet: Enter Microsoft's Automatic Prompt Optimization (APO) framework. Introduced in a recent paper from Microsoft AI Research [2], APO represents a significant step towards automating the prompt engineering process in a generalizable and efficient way. ... APO promises to greatly expand the accessibility and impact of these cutting-edge tools. How ..., title: Microsoft's Automatic Prompt Optimization Framework: A Leap Forward for ..., link: https://www.33rdsquare.com/microsoft-introduces-automatic-prompt-optimization-framework-for-llms/, snippet: PromptWizard (PW) is designed to automate and simplify prompt optimization. It combines iterative feedback from LLMs with efficient exploration and refinement techniques to create highly effective prompts within minutes. PromptWizard optimizes both the instruction and the in-context learning examples., title: PromptWizard: The future of prompt optimization through feedback-driven ..., link: https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/\", '#E5': 'content=\"Here\\'s a detailed step-by-step guide on how to automatically optimize prompts for Large Language Models (LLMs):\\\\n\\\\n1. Define Optimization Objectives:\\\\n   - Clearly specify the task and desired outcomes (e.g., accuracy, relevance, specificity).\\\\n   - Determine evaluation metrics (e.g., BLEU score for translation, F1 score for classification).\\\\n\\\\n2. Prepare Dataset:\\\\n   - Collect a diverse set of input-output pairs relevant to the task.\\\\n   - Include both clean and perturbed data to ensure robustness (as suggested by BATprompt).\\\\n   - Split data into training, validation, and test sets.\\\\n\\\\n3. Initialize Prompt Templates:\\\\n   - Create a set of initial prompts or templates based on task requirements.\\\\n   - Include placeholders for dynamic elements (e.g., input text, examples).\\\\n\\\\n4. Set Up Optimization Framework:\\\\n   - Choose an optimization algorithm (e.g., gradient-based methods, evolutionary algorithms).\\\\n   - Implement or use existing frameworks like Microsoft\\'s SAMMO or PromptWizard.\\\\n\\\\n5. Iterative Optimization Process:\\\\n   a. Generate Candidate Prompts:\\\\n      - Use techniques like genetic algorithms or LLM-based generation (as in OPRO).\\\\n      - Apply mutations or crossovers to existing prompts.\\\\n\\\\n   b. Evaluate Prompts:\\\\n      - Run candidates through the target LLM.\\\\n      - Compute performance metrics on validation set.\\\\n\\\\n   c. Select and Refine:\\\\n      - Choose top-performing prompts.\\\\n      - Apply techniques like beam search or simulated annealing to explore variations.\\\\n\\\\n   d. Update Optimization Parameters:\\\\n      - Adjust learning rates, mutation probabilities, or other hyperparameters.\\\\n      - Implement adaptive strategies like those used in Promptbreeder.\\\\n\\\\n6. Incorporate LLM Feedback:\\\\n   - Use the LLM itself to generate feedback on prompt quality.\\\\n   - Implement self-reflection mechanisms to improve prompts (as in PromptWizard).\\\\n\\\\n7. Robustness Testing:\\\\n   - Evaluate prompts on perturbed inputs (as suggested by BATprompt).\\\\n   - Ensure consistent performance across various input conditions.\\\\n\\\\n8. Fine-tune for Specific Models:\\\\n   - Adapt optimization process for different LLMs (e.g., GPT-3.5, Llama-2, Mixtral).\\\\n   - Consider model-specific quirks and capabilities.\\\\n\\\\n9. Human-in-the-Loop Refinement:\\\\n   - Periodically review and manually adjust top-performing prompts.\\\\n   - Incorporate domain expertise to guide optimization.\\\\n\\\\n10. Implement Stopping Criteria:\\\\n    - Set performance thresholds or maximum iteration limits.\\\\n    - Use techniques like early stopping to prevent overfitting.\\\\n\\\\n11. Final Evaluation and Selection:\\\\n    - Test top prompts on held-out test set.\\\\n    - Consider trade-offs between performance, length, and complexity.\\\\n\\\\n12. Documentation and Deployment:\\\\n    - Record optimization process, parameters, and results.\\\\n    - Integrate optimized prompts into production systems or AI platforms (e.g., Vertex AI Prompt Optimizer).\\\\n\\\\nTools and Techniques:\\\\n- Use frameworks like SAMMO, PromptWizard, or Vertex AI Prompt Optimizer.\\\\n- Implement algorithms like Automatic Prompt Optimization (APO) or GrIPS.\\\\n- Consider Bayesian optimization approaches for efficient exploration.\\\\n- Utilize LLMs themselves as part of the optimization process (e.g., OPRO method).\\\\n\\\\nRemember that while these steps provide a comprehensive guide, the specific implementation may vary based on the task, available resources, and chosen optimization framework. Continuously monitor the field for new research and techniques to improve your prompt optimization process.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01UchpbdxD7kbBuKifdGeqvn\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 2210, \\'output_tokens\\': 870}} id=\\'run-60650923-aac0-4e5c-8a56-c9550a86dd39-0\\' usage_metadata={\\'input_tokens\\': 2210, \\'output_tokens\\': 870, \\'total_tokens\\': 3080, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer', '#E2': 'content=\"Based on the information provided in the snippets, the main components or steps involved in automatic prompt optimization for Large Language Models (LLMs) typically include:\\\\n\\\\n1. Input Data Preparation: The process starts with preparing input data, which may include both clean and potentially perturbed data to ensure robustness.\\\\n\\\\n2. Prompt Generation: Initial prompts are generated, either manually or through automated means, as a starting point for optimization.\\\\n\\\\n3. Iterative Optimization: The prompts undergo an iterative process of refinement and improvement. This may involve:\\\\n   - Testing prompts against the model\\\\n   - Evaluating performance based on predefined metrics\\\\n   - Adjusting prompts based on results\\\\n\\\\n4. Performance Evaluation: The quality of prompts is assessed based on the semantic and structural integrity of the output, as well as the model\\'s performance on various tasks.\\\\n\\\\n5. Robustness Testing: As mentioned in the BATprompt snippet, testing prompts against perturbed inputs to ensure consistent performance across various input conditions.\\\\n\\\\n6. Human Oversight: Despite automation, human prompt engineers still play a crucial role in overseeing the process and making high-level decisions.\\\\n\\\\n7. Integration with AI Platforms: As seen in the Vertex AI Prompt Optimizer example, these techniques can be integrated into larger AI platforms for easier access and use.\\\\n\\\\n8. Adaptation to Specific Models: The optimization process may need to be tailored to the specific LLM being used, as different models may respond differently to prompts.\\\\n\\\\n9. Incorporation of Research Findings: The process is continually evolving based on new research, such as the APO methods mentioned in the Google Cloud snippet.\\\\n\\\\nIt\\'s important to note that while these systems aim to automate much of the prompt optimization process, they are generally designed to assist rather than replace human prompt engineers, serving as a tool to enhance efficiency and performance in prompt development for LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01Nxc84zUqNRxA1XNGs1F8NX\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 519, \\'output_tokens\\': 419}} id=\\'run-faeebf31-6288-4b24-af12-68646560c7b0-0\\' usage_metadata={\\'input_tokens\\': 519, \\'output_tokens\\': 419, \\'total_tokens\\': 938, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E3': 'snippet: LLMs are good prompt engineers. ... While other prompt optimization algorithms commonly see diminishing returns and a saturation in performance when extending the optimization process to larger numbers of iterations [1], Promptbreeder dynamically adapts the optimization process over time, allowing intricate prompts to be discovered that are ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: This process is illustrated in Figure 1, which shows a RAG prompt designed to translate user queries into a domain-specific language (DSL), also known as semantic parsing. Figure 1: A RAG prompt is used for a semantic parsing task. The underlying prompt consists of three larger parts, each with a variety of aspects that can be optimized., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: The performance of Large Language Models (LLMs) in a specific task is critically dependent upon the instruction (prompt) received. Prompt optimization aims at selecting a sequence of tokens which concatenated with the text query yields an instruction which optimizes..., title: A Bayesian Approach for Prompt Optimization in LLMs, link: https://link.springer.com/chapter/10.1007/978-3-031-75623-8_28, snippet: 2 Opro: Llm as the Optimizer and 2.1 Desirables of Optimization by Llms. 2.2 Meta-Prompt Design. 3 Motivating Example: Mathematical Optimization and 3.1 Linear Regression. 3.2 Traveling Salesman Problem (TSP) ... OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human ..., title: Optimizing Prompts with LLMs: Key Findings and Future Directions, link: https://hackernoon.com/optimizing-prompts-with-llms-key-findings-and-future-directions', '#E4': \"snippet: To align with previous research, we used eight zero-shot BigBench classification tasks where the baseline prompt for GPT-3.5 achieved an accuracy of less than 0.9. We compared it against Automatic Prompt Optimization (APO) and GrIPS, applying open-source models Mixtral 7x8B and Llama-2 70B, alongside GPT-3.5 as backend LLMs., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving ..., title: PromptWizard: Task-Aware Prompt Optimization Framework, link: https://arxiv.org/abs/2405.18369, snippet: Enter Microsoft's Automatic Prompt Optimization (APO) framework. Introduced in a recent paper from Microsoft AI Research [2], APO represents a significant step towards automating the prompt engineering process in a generalizable and efficient way. ... APO promises to greatly expand the accessibility and impact of these cutting-edge tools. How ..., title: Microsoft's Automatic Prompt Optimization Framework: A Leap Forward for ..., link: https://www.33rdsquare.com/microsoft-introduces-automatic-prompt-optimization-framework-for-llms/, snippet: PromptWizard (PW) is designed to automate and simplify prompt optimization. It combines iterative feedback from LLMs with efficient exploration and refinement techniques to create highly effective prompts within minutes. PromptWizard optimizes both the instruction and the in-context learning examples., title: PromptWizard: The future of prompt optimization through feedback-driven ..., link: https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/\", '#E5': 'content=\"Here\\'s a detailed step-by-step guide on how to automatically optimize prompts for Large Language Models (LLMs):\\\\n\\\\n1. Define Optimization Objectives:\\\\n   - Clearly specify the task and desired outcomes (e.g., accuracy, relevance, specificity).\\\\n   - Determine evaluation metrics (e.g., BLEU score for translation, F1 score for classification).\\\\n\\\\n2. Prepare Dataset:\\\\n   - Collect a diverse set of input-output pairs relevant to the task.\\\\n   - Include both clean and perturbed data to ensure robustness (as suggested by BATprompt).\\\\n   - Split data into training, validation, and test sets.\\\\n\\\\n3. Initialize Prompt Templates:\\\\n   - Create a set of initial prompts or templates based on task requirements.\\\\n   - Include placeholders for dynamic elements (e.g., input text, examples).\\\\n\\\\n4. Set Up Optimization Framework:\\\\n   - Choose an optimization algorithm (e.g., gradient-based methods, evolutionary algorithms).\\\\n   - Implement or use existing frameworks like Microsoft\\'s SAMMO or PromptWizard.\\\\n\\\\n5. Iterative Optimization Process:\\\\n   a. Generate Candidate Prompts:\\\\n      - Use techniques like genetic algorithms or LLM-based generation (as in OPRO).\\\\n      - Apply mutations or crossovers to existing prompts.\\\\n\\\\n   b. Evaluate Prompts:\\\\n      - Run candidates through the target LLM.\\\\n      - Compute performance metrics on validation set.\\\\n\\\\n   c. Select and Refine:\\\\n      - Choose top-performing prompts.\\\\n      - Apply techniques like beam search or simulated annealing to explore variations.\\\\n\\\\n   d. Update Optimization Parameters:\\\\n      - Adjust learning rates, mutation probabilities, or other hyperparameters.\\\\n      - Implement adaptive strategies like those used in Promptbreeder.\\\\n\\\\n6. Incorporate LLM Feedback:\\\\n   - Use the LLM itself to generate feedback on prompt quality.\\\\n   - Implement self-reflection mechanisms to improve prompts (as in PromptWizard).\\\\n\\\\n7. Robustness Testing:\\\\n   - Evaluate prompts on perturbed inputs (as suggested by BATprompt).\\\\n   - Ensure consistent performance across various input conditions.\\\\n\\\\n8. Fine-tune for Specific Models:\\\\n   - Adapt optimization process for different LLMs (e.g., GPT-3.5, Llama-2, Mixtral).\\\\n   - Consider model-specific quirks and capabilities.\\\\n\\\\n9. Human-in-the-Loop Refinement:\\\\n   - Periodically review and manually adjust top-performing prompts.\\\\n   - Incorporate domain expertise to guide optimization.\\\\n\\\\n10. Implement Stopping Criteria:\\\\n    - Set performance thresholds or maximum iteration limits.\\\\n    - Use techniques like early stopping to prevent overfitting.\\\\n\\\\n11. Final Evaluation and Selection:\\\\n    - Test top prompts on held-out test set.\\\\n    - Consider trade-offs between performance, length, and complexity.\\\\n\\\\n12. Documentation and Deployment:\\\\n    - Record optimization process, parameters, and results.\\\\n    - Integrate optimized prompts into production systems or AI platforms (e.g., Vertex AI Prompt Optimizer).\\\\n\\\\nTools and Techniques:\\\\n- Use frameworks like SAMMO, PromptWizard, or Vertex AI Prompt Optimizer.\\\\n- Implement algorithms like Automatic Prompt Optimization (APO) or GrIPS.\\\\n- Consider Bayesian optimization approaches for efficient exploration.\\\\n- Utilize LLMs themselves as part of the optimization process (e.g., OPRO method).\\\\n\\\\nRemember that while these steps provide a comprehensive guide, the specific implementation may vary based on the task, available resources, and chosen optimization framework. Continuously monitor the field for new research and techniques to improve your prompt optimization process.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01UchpbdxD7kbBuKifdGeqvn\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 2210, \\'output_tokens\\': 870}} id=\\'run-60650923-aac0-4e5c-8a56-c9550a86dd39-0\\' usage_metadata={\\'input_tokens\\': 2210, \\'output_tokens\\': 870, \\'total_tokens\\': 3080, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E6': 'snippet: Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly ..., title: The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for ..., link: https://arxiv.org/abs/2501.01329, snippet: In a nutshell, MAPS is a state-of-the-art optimization technique for prompt generation, particularly targeted to LLMs used in the software testing domain. Some of the weaknesses of the available test case generation techniques have been addressed through multi-pipeline-stage architectures that incorporate baseline evaluations, iterative ..., title: The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for ..., link: https://www.marktechpost.com/2025/01/08/the-prompt-alchemist-automated-llm-tailored-prompt-optimization-for-test-case-generation/, snippet: Recent studies have proposed to utilize LLMs as strategy planners or optimizers in concrete optimization tasks. ... and TCP represents one of the state-of-the-art advancements in this realm. Both methods exemplify the use of textual prompting techniques for enhancing vision-language models. ... Automatic prompt optimization with \"gradient ..., title: LLM as a Complementary Optimizer to Gradient Descent: A Case Study in ..., link: https://arxiv.org/html/2405.19732v4, snippet: Automatic prompt engineering aims to enhance the generation quality of large language models (LLMs). Recent works utilize feedbacks generated from erroneous cases to guide the prompt optimization. During inference, they may further retrieve several semantically-related exemplars and concatenate them to the optimized prompts to improve the performance. However, those works only utilize the ..., title: Efficient and Accurate Prompt Optimization: the Benefit of Memory in ..., link: https://arxiv.org/abs/2411.07446'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'snippet: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\\\\\\\\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic ..., title: iPrOp: Interactive Prompt Optimization for Large Language Models with a ..., link: https://arxiv.org/abs/2412.12644, snippet: The performance of Large Language Models (LLMs) is based on the quality of the prompts and the semantic and structural integrity information of the input data. However, current prompt generation methods primarily focus on generating prompts for clean input data, often overlooking the impact of perturbed inputs on prompt performance. To address this limitation, we propose BATprompt (By ..., title: [2412.18196] Robustness-aware Automatic Prompt Optimization - arXiv.org, link: https://arxiv.org/abs/2412.18196, snippet: Put simply, automatic prompt optimization techniques are assistive in nature. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. ... Laria, and Kyle McDonell. \"Prompt programming for large language models: Beyond the few-shot paradigm.\" Extended ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: Optimize large language model prompts with Vertex AI Prompt Optimizer. Learn how to tailor prompts, enhance model output, and save time on manual optimization. ... for any preferred model on Vertex AI. It is based on Google Research\\'s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an ..., title: Announcing Public Preview of Vertex AI Prompt Optimizer - Google Cloud, link: https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer', '#E2': 'content=\"Based on the information provided in the snippets, the main components or steps involved in automatic prompt optimization for Large Language Models (LLMs) typically include:\\\\n\\\\n1. Input Data Preparation: The process starts with preparing input data, which may include both clean and potentially perturbed data to ensure robustness.\\\\n\\\\n2. Prompt Generation: Initial prompts are generated, either manually or through automated means, as a starting point for optimization.\\\\n\\\\n3. Iterative Optimization: The prompts undergo an iterative process of refinement and improvement. This may involve:\\\\n   - Testing prompts against the model\\\\n   - Evaluating performance based on predefined metrics\\\\n   - Adjusting prompts based on results\\\\n\\\\n4. Performance Evaluation: The quality of prompts is assessed based on the semantic and structural integrity of the output, as well as the model\\'s performance on various tasks.\\\\n\\\\n5. Robustness Testing: As mentioned in the BATprompt snippet, testing prompts against perturbed inputs to ensure consistent performance across various input conditions.\\\\n\\\\n6. Human Oversight: Despite automation, human prompt engineers still play a crucial role in overseeing the process and making high-level decisions.\\\\n\\\\n7. Integration with AI Platforms: As seen in the Vertex AI Prompt Optimizer example, these techniques can be integrated into larger AI platforms for easier access and use.\\\\n\\\\n8. Adaptation to Specific Models: The optimization process may need to be tailored to the specific LLM being used, as different models may respond differently to prompts.\\\\n\\\\n9. Incorporation of Research Findings: The process is continually evolving based on new research, such as the APO methods mentioned in the Google Cloud snippet.\\\\n\\\\nIt\\'s important to note that while these systems aim to automate much of the prompt optimization process, they are generally designed to assist rather than replace human prompt engineers, serving as a tool to enhance efficiency and performance in prompt development for LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01Nxc84zUqNRxA1XNGs1F8NX\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 519, \\'output_tokens\\': 419}} id=\\'run-faeebf31-6288-4b24-af12-68646560c7b0-0\\' usage_metadata={\\'input_tokens\\': 519, \\'output_tokens\\': 419, \\'total_tokens\\': 938, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E3': 'snippet: LLMs are good prompt engineers. ... While other prompt optimization algorithms commonly see diminishing returns and a saturation in performance when extending the optimization process to larger numbers of iterations [1], Promptbreeder dynamically adapts the optimization process over time, allowing intricate prompts to be discovered that are ..., title: Automatic Prompt Optimization - by Cameron R. Wolfe, Ph.D., link: https://cameronrwolfe.substack.com/p/automatic-prompt-optimization, snippet: This process is illustrated in Figure 1, which shows a RAG prompt designed to translate user queries into a domain-specific language (DSL), also known as semantic parsing. Figure 1: A RAG prompt is used for a semantic parsing task. The underlying prompt consists of three larger parts, each with a variety of aspects that can be optimized., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: The performance of Large Language Models (LLMs) in a specific task is critically dependent upon the instruction (prompt) received. Prompt optimization aims at selecting a sequence of tokens which concatenated with the text query yields an instruction which optimizes..., title: A Bayesian Approach for Prompt Optimization in LLMs, link: https://link.springer.com/chapter/10.1007/978-3-031-75623-8_28, snippet: 2 Opro: Llm as the Optimizer and 2.1 Desirables of Optimization by Llms. 2.2 Meta-Prompt Design. 3 Motivating Example: Mathematical Optimization and 3.1 Linear Regression. 3.2 Traveling Salesman Problem (TSP) ... OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human ..., title: Optimizing Prompts with LLMs: Key Findings and Future Directions, link: https://hackernoon.com/optimizing-prompts-with-llms-key-findings-and-future-directions', '#E4': \"snippet: To align with previous research, we used eight zero-shot BigBench classification tasks where the baseline prompt for GPT-3.5 achieved an accuracy of less than 0.9. We compared it against Automatic Prompt Optimization (APO) and GrIPS, applying open-source models Mixtral 7x8B and Llama-2 70B, alongside GPT-3.5 as backend LLMs., title: SAMMO: A general-purpose framework for prompt optimization, link: https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/, snippet: Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving ..., title: PromptWizard: Task-Aware Prompt Optimization Framework, link: https://arxiv.org/abs/2405.18369, snippet: Enter Microsoft's Automatic Prompt Optimization (APO) framework. Introduced in a recent paper from Microsoft AI Research [2], APO represents a significant step towards automating the prompt engineering process in a generalizable and efficient way. ... APO promises to greatly expand the accessibility and impact of these cutting-edge tools. How ..., title: Microsoft's Automatic Prompt Optimization Framework: A Leap Forward for ..., link: https://www.33rdsquare.com/microsoft-introduces-automatic-prompt-optimization-framework-for-llms/, snippet: PromptWizard (PW) is designed to automate and simplify prompt optimization. It combines iterative feedback from LLMs with efficient exploration and refinement techniques to create highly effective prompts within minutes. PromptWizard optimizes both the instruction and the in-context learning examples., title: PromptWizard: The future of prompt optimization through feedback-driven ..., link: https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/\", '#E5': 'content=\"Here\\'s a detailed step-by-step guide on how to automatically optimize prompts for Large Language Models (LLMs):\\\\n\\\\n1. Define Optimization Objectives:\\\\n   - Clearly specify the task and desired outcomes (e.g., accuracy, relevance, specificity).\\\\n   - Determine evaluation metrics (e.g., BLEU score for translation, F1 score for classification).\\\\n\\\\n2. Prepare Dataset:\\\\n   - Collect a diverse set of input-output pairs relevant to the task.\\\\n   - Include both clean and perturbed data to ensure robustness (as suggested by BATprompt).\\\\n   - Split data into training, validation, and test sets.\\\\n\\\\n3. Initialize Prompt Templates:\\\\n   - Create a set of initial prompts or templates based on task requirements.\\\\n   - Include placeholders for dynamic elements (e.g., input text, examples).\\\\n\\\\n4. Set Up Optimization Framework:\\\\n   - Choose an optimization algorithm (e.g., gradient-based methods, evolutionary algorithms).\\\\n   - Implement or use existing frameworks like Microsoft\\'s SAMMO or PromptWizard.\\\\n\\\\n5. Iterative Optimization Process:\\\\n   a. Generate Candidate Prompts:\\\\n      - Use techniques like genetic algorithms or LLM-based generation (as in OPRO).\\\\n      - Apply mutations or crossovers to existing prompts.\\\\n\\\\n   b. Evaluate Prompts:\\\\n      - Run candidates through the target LLM.\\\\n      - Compute performance metrics on validation set.\\\\n\\\\n   c. Select and Refine:\\\\n      - Choose top-performing prompts.\\\\n      - Apply techniques like beam search or simulated annealing to explore variations.\\\\n\\\\n   d. Update Optimization Parameters:\\\\n      - Adjust learning rates, mutation probabilities, or other hyperparameters.\\\\n      - Implement adaptive strategies like those used in Promptbreeder.\\\\n\\\\n6. Incorporate LLM Feedback:\\\\n   - Use the LLM itself to generate feedback on prompt quality.\\\\n   - Implement self-reflection mechanisms to improve prompts (as in PromptWizard).\\\\n\\\\n7. Robustness Testing:\\\\n   - Evaluate prompts on perturbed inputs (as suggested by BATprompt).\\\\n   - Ensure consistent performance across various input conditions.\\\\n\\\\n8. Fine-tune for Specific Models:\\\\n   - Adapt optimization process for different LLMs (e.g., GPT-3.5, Llama-2, Mixtral).\\\\n   - Consider model-specific quirks and capabilities.\\\\n\\\\n9. Human-in-the-Loop Refinement:\\\\n   - Periodically review and manually adjust top-performing prompts.\\\\n   - Incorporate domain expertise to guide optimization.\\\\n\\\\n10. Implement Stopping Criteria:\\\\n    - Set performance thresholds or maximum iteration limits.\\\\n    - Use techniques like early stopping to prevent overfitting.\\\\n\\\\n11. Final Evaluation and Selection:\\\\n    - Test top prompts on held-out test set.\\\\n    - Consider trade-offs between performance, length, and complexity.\\\\n\\\\n12. Documentation and Deployment:\\\\n    - Record optimization process, parameters, and results.\\\\n    - Integrate optimized prompts into production systems or AI platforms (e.g., Vertex AI Prompt Optimizer).\\\\n\\\\nTools and Techniques:\\\\n- Use frameworks like SAMMO, PromptWizard, or Vertex AI Prompt Optimizer.\\\\n- Implement algorithms like Automatic Prompt Optimization (APO) or GrIPS.\\\\n- Consider Bayesian optimization approaches for efficient exploration.\\\\n- Utilize LLMs themselves as part of the optimization process (e.g., OPRO method).\\\\n\\\\nRemember that while these steps provide a comprehensive guide, the specific implementation may vary based on the task, available resources, and chosen optimization framework. Continuously monitor the field for new research and techniques to improve your prompt optimization process.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01UchpbdxD7kbBuKifdGeqvn\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 2210, \\'output_tokens\\': 870}} id=\\'run-60650923-aac0-4e5c-8a56-c9550a86dd39-0\\' usage_metadata={\\'input_tokens\\': 2210, \\'output_tokens\\': 870, \\'total_tokens\\': 3080, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}', '#E6': 'snippet: Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly ..., title: The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for ..., link: https://arxiv.org/abs/2501.01329, snippet: In a nutshell, MAPS is a state-of-the-art optimization technique for prompt generation, particularly targeted to LLMs used in the software testing domain. Some of the weaknesses of the available test case generation techniques have been addressed through multi-pipeline-stage architectures that incorporate baseline evaluations, iterative ..., title: The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for ..., link: https://www.marktechpost.com/2025/01/08/the-prompt-alchemist-automated-llm-tailored-prompt-optimization-for-test-case-generation/, snippet: Recent studies have proposed to utilize LLMs as strategy planners or optimizers in concrete optimization tasks. ... and TCP represents one of the state-of-the-art advancements in this realm. Both methods exemplify the use of textual prompting techniques for enhancing vision-language models. ... Automatic prompt optimization with \"gradient ..., title: LLM as a Complementary Optimizer to Gradient Descent: A Case Study in ..., link: https://arxiv.org/html/2405.19732v4, snippet: Automatic prompt engineering aims to enhance the generation quality of large language models (LLMs). Recent works utilize feedbacks generated from erroneous cases to guide the prompt optimization. During inference, they may further retrieve several semantically-related exemplars and concatenate them to the optimized prompts to improve the performance. However, those works only utilize the ..., title: Efficient and Accurate Prompt Optimization: the Benefit of Memory in ..., link: https://arxiv.org/abs/2411.07446', '#E7': 'content=\"Here\\'s an updated guide incorporating recent advancements and practical examples of automatic prompt optimization for LLMs, particularly focusing on test case generation and software testing:\\\\n\\\\n1. Define Optimization Objectives:\\\\n   - Specify the task (e.g., generating test cases for software applications).\\\\n   - Determine evaluation metrics (e.g., code coverage, fault detection rate).\\\\n\\\\n2. Prepare Dataset:\\\\n   - Collect diverse source code samples and corresponding test cases.\\\\n   - Include both clean and perturbed code to ensure robustness.\\\\n   - Split data into training, validation, and test sets.\\\\n\\\\n3. Initialize Prompt Templates:\\\\n   - Create initial prompts based on software testing best practices.\\\\n   - Include placeholders for code snippets and specific testing requirements.\\\\n\\\\n4. Set Up Optimization Framework:\\\\n   - Implement a multi-pipeline-stage architecture like MAPS (Multi-stage Automatic Prompt Synthesis).\\\\n   - Incorporate baseline evaluations and iterative refinement stages.\\\\n\\\\n5. Iterative Optimization Process:\\\\n   a. Generate Candidate Prompts:\\\\n      - Use LLM-based generation techniques.\\\\n      - Apply mutations or crossovers to existing prompts.\\\\n\\\\n   b. Evaluate Prompts:\\\\n      - Run candidates through the target LLM to generate test cases.\\\\n      - Compute performance metrics (e.g., code coverage, fault detection rate).\\\\n\\\\n   c. Select and Refine:\\\\n      - Choose top-performing prompts.\\\\n      - Apply techniques like beam search or simulated annealing.\\\\n\\\\n   d. Update Optimization Parameters:\\\\n      - Adjust hyperparameters based on performance feedback.\\\\n\\\\n6. Incorporate LLM Feedback:\\\\n   - Use the LLM to analyze generated test cases and provide improvement suggestions.\\\\n   - Implement self-reflection mechanisms to enhance prompt quality.\\\\n\\\\n7. Robustness Testing:\\\\n   - Evaluate prompts on perturbed code samples.\\\\n   - Ensure consistent performance across various programming languages and paradigms.\\\\n\\\\n8. Fine-tune for Specific Models:\\\\n   - Adapt optimization process for different LLMs used in software testing.\\\\n   - Consider model-specific capabilities related to code understanding.\\\\n\\\\n9. Human-in-the-Loop Refinement:\\\\n   - Periodically review generated test cases with experienced QA engineers.\\\\n   - Incorporate domain expertise to guide optimization towards industry best practices.\\\\n\\\\n10. Implement Stopping Criteria:\\\\n    - Set performance thresholds based on industry standards for test coverage and quality.\\\\n    - Use early stopping to prevent overfitting to specific code patterns.\\\\n\\\\n11. Final Evaluation and Selection:\\\\n    - Test top prompts on held-out test set of complex software projects.\\\\n    - Consider trade-offs between test case quality, coverage, and generation speed.\\\\n\\\\n12. Documentation and Deployment:\\\\n    - Record optimization process, parameters, and results for reproducibility.\\\\n    - Integrate optimized prompts into existing software testing pipelines or CI/CD systems.\\\\n\\\\nRecent Advancements and Techniques:\\\\n- Utilize MAPS (Multi-stage Automatic Prompt Synthesis) for a more structured optimization approach.\\\\n- Implement memory-based prompt optimization techniques to improve efficiency and accuracy.\\\\n- Explore the use of LLMs as complementary optimizers to gradient descent methods.\\\\n- Incorporate semantic-related exemplar retrieval during inference to enhance performance.\\\\n- Consider adapting vision-language model techniques like TCP (Text Conditioning Prompt) for code-related tasks.\\\\n\\\\nPractical Considerations:\\\\n- Focus on generating diverse test cases that cover edge cases and potential vulnerabilities.\\\\n- Optimize prompts to generate test cases that align with specific testing frameworks or methodologies used in the development process.\\\\n- Continuously update the optimization process with new research in software testing and LLM capabilities.\\\\n- Benchmark the automated test case generation against human-written tests to ensure quality and reliability.\\\\n\\\\nBy implementing these advanced techniques and focusing on the specific domain of software testing, you can create a more effective and tailored prompt optimization process for generating high-quality test cases using LLMs.\" additional_kwargs={} response_metadata={\\'id\\': \\'msg_01VjnBjGUPjJb1PPj7KbNSwQ\\', \\'model\\': \\'claude-3-5-sonnet-20240620\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'usage\\': {\\'cache_creation_input_tokens\\': 0, \\'cache_read_input_tokens\\': 0, \\'input_tokens\\': 1654, \\'output_tokens\\': 911}} id=\\'run-348ce103-d9ec-4a89-8e3c-8a5195969c8b-0\\' usage_metadata={\\'input_tokens\\': 1654, \\'output_tokens\\': 911, \\'total_tokens\\': 2565, \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}}'}}}\n",
      "---\n",
      "{'solve': {'result': \"Here's a guide to automatically optimize prompts for LLMs, incorporating recent advancements:\\n\\n1. Define Optimization Objectives:\\n   - Specify task (e.g., generating test cases for software)\\n   - Determine evaluation metrics (e.g., code coverage, fault detection rate)\\n\\n2. Prepare Dataset:\\n   - Collect diverse input-output pairs (e.g., source code and test cases)\\n   - Include clean and perturbed data for robustness\\n   - Split into training, validation, and test sets\\n\\n3. Initialize Prompt Templates:\\n   - Create initial prompts based on task requirements\\n   - Include placeholders for dynamic elements\\n\\n4. Set Up Optimization Framework:\\n   - Implement multi-pipeline-stage architecture (e.g., MAPS)\\n   - Incorporate baseline evaluations and iterative refinement\\n\\n5. Iterative Optimization Process:\\n   a. Generate Candidate Prompts:\\n      - Use LLM-based generation or genetic algorithms\\n      - Apply mutations or crossovers\\n   b. Evaluate Prompts:\\n      - Run through target LLM\\n      - Compute performance metrics\\n   c. Select and Refine:\\n      - Choose top performers\\n      - Apply beam search or simulated annealing\\n   d. Update Parameters:\\n      - Adjust hyperparameters based on feedback\\n\\n6. Incorporate LLM Feedback:\\n   - Use LLM to analyze outputs and suggest improvements\\n   - Implement self-reflection mechanisms\\n\\n7. Robustness Testing:\\n   - Evaluate on perturbed inputs\\n   - Ensure consistent performance across conditions\\n\\n8. Fine-tune for Specific Models:\\n   - Adapt process for different LLMs\\n   - Consider model-specific capabilities\\n\\n9. Human-in-the-Loop Refinement:\\n   - Periodically review with domain experts\\n   - Incorporate expert feedback\\n\\n10. Implement Stopping Criteria:\\n    - Set performance thresholds\\n    - Use early stopping to prevent overfitting\\n\\n11. Final Evaluation and Selection:\\n    - Test on held-out set\\n    - Consider performance trade-offs\\n\\n12. Documentation and Deployment:\\n    - Record process, parameters, and results\\n    - Integrate into production systems\\n\\nRecent Techniques:\\n- Use MAPS for structured optimization\\n- Implement memory-based optimization for efficiency\\n- Explore LLMs as complementary optimizers to gradient descent\\n- Incorporate semantic-related exemplar retrieval\\n- Adapt vision-language model techniques for code tasks\\n\\nContinuously update the process with new research and benchmark against human-generated results.\"}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for s in app.stream({\"task\": task}):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a guide to automatically optimize prompts for LLMs, incorporating recent advancements:\n",
      "\n",
      "1. Define Optimization Objectives:\n",
      "   - Specify task (e.g., generating test cases for software)\n",
      "   - Determine evaluation metrics (e.g., code coverage, fault detection rate)\n",
      "\n",
      "2. Prepare Dataset:\n",
      "   - Collect diverse input-output pairs (e.g., source code and test cases)\n",
      "   - Include clean and perturbed data for robustness\n",
      "   - Split into training, validation, and test sets\n",
      "\n",
      "3. Initialize Prompt Templates:\n",
      "   - Create initial prompts based on task requirements\n",
      "   - Include placeholders for dynamic elements\n",
      "\n",
      "4. Set Up Optimization Framework:\n",
      "   - Implement multi-pipeline-stage architecture (e.g., MAPS)\n",
      "   - Incorporate baseline evaluations and iterative refinement\n",
      "\n",
      "5. Iterative Optimization Process:\n",
      "   a. Generate Candidate Prompts:\n",
      "      - Use LLM-based generation or genetic algorithms\n",
      "      - Apply mutations or crossovers\n",
      "   b. Evaluate Prompts:\n",
      "      - Run through target LLM\n",
      "      - Compute performance metrics\n",
      "   c. Select and Refine:\n",
      "      - Choose top performers\n",
      "      - Apply beam search or simulated annealing\n",
      "   d. Update Parameters:\n",
      "      - Adjust hyperparameters based on feedback\n",
      "\n",
      "6. Incorporate LLM Feedback:\n",
      "   - Use LLM to analyze outputs and suggest improvements\n",
      "   - Implement self-reflection mechanisms\n",
      "\n",
      "7. Robustness Testing:\n",
      "   - Evaluate on perturbed inputs\n",
      "   - Ensure consistent performance across conditions\n",
      "\n",
      "8. Fine-tune for Specific Models:\n",
      "   - Adapt process for different LLMs\n",
      "   - Consider model-specific capabilities\n",
      "\n",
      "9. Human-in-the-Loop Refinement:\n",
      "   - Periodically review with domain experts\n",
      "   - Incorporate expert feedback\n",
      "\n",
      "10. Implement Stopping Criteria:\n",
      "    - Set performance thresholds\n",
      "    - Use early stopping to prevent overfitting\n",
      "\n",
      "11. Final Evaluation and Selection:\n",
      "    - Test on held-out set\n",
      "    - Consider performance trade-offs\n",
      "\n",
      "12. Documentation and Deployment:\n",
      "    - Record process, parameters, and results\n",
      "    - Integrate into production systems\n",
      "\n",
      "Recent Techniques:\n",
      "- Use MAPS for structured optimization\n",
      "- Implement memory-based optimization for efficiency\n",
      "- Explore LLMs as complementary optimizers to gradient descent\n",
      "- Incorporate semantic-related exemplar retrieval\n",
      "- Adapt vision-language model techniques for code tasks\n",
      "\n",
      "Continuously update the process with new research and benchmark against human-generated results.\n"
     ]
    }
   ],
   "source": [
    "# Print out the final result\n",
    "print(s[\"solve\"][\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
