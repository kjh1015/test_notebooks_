{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DNSOFT\\AppData\\Local\\Temp\\ipykernel_29416\\3918532883.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm1 = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Initialize two LLMs\n",
    "llm1 = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
    "llm2 = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation between LLM1 and LLM2 with personas set in system messages...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DNSOFT\\AppData\\Local\\Temp\\ipykernel_29416\\2924390786.py:85: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response1 = llm1([system_message1, HumanMessage(content=initial_prompt)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM1: 안녕\n",
      "LLM2: {\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 0,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it a type of fruit? (과일의 종류인가요?)\",\n",
      "        \"Is it red? (빨간색인가요?)\",\n",
      "        \"Is it commonly eaten raw? (보통 생으로 먹나요?)\",\n",
      "        \"Does it grow on trees? (나무에서 자주 자라나요?)\",\n",
      "        \"Is it sweet? (단맛이 있나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": false,\n",
      "    \"is_end\": false,\n",
      "    \"message\": \"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요. Is it a type of fruit? (과일의 종류인가요?)\"\n",
      "}\n",
      "\n",
      "LLM1: {\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 0,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it a type of fruit? (과일의 종류인가요?)\",\n",
      "        \"Is it red? (빨간색인가요?)\",\n",
      "        \"Is it commonly eaten raw? (보통 생으로 먹나요?)\",\n",
      "        \"Does it grow on trees? (나무에서 자주 자라나요?)\",\n",
      "        \"Is it sweet? (단맛이 있나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": false,\n",
      "    \"is_end\": false,\n",
      "    \"message\": \"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요. Is it a type of fruit? (과일의 종류인가요?)\"\n",
      "}\n",
      "LLM2: 좋아요! 제가 맞추고 있는 것은 \"사과\"입니다. 제 생각이 맞는지 확인해 볼까요? 질문에 대한 답변을 주시면 다음 단계로 진행할 수 있어요. 현재 질문은 \"Is it a type of fruit? (과일의 종류인가요?)\"입니다. 답변해 주시면 계속 진행할게요!\n",
      "\n",
      "LLM2: 좋아요! 제가 맞추고 있는 것은 \"사과\"입니다. 제 생각이 맞는지 확인해 볼까요? 질문에 대한 답변을 주시면 다음 단계로 진행할 수 있어요. 현재 질문은 \"Is it a type of fruit? (과일의 종류인가요?)\"입니다. 답변해 주시면 계속 진행할게요!\n",
      "LLM1: {\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 0,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it red or green? (빨간색인가요, 아니면 초록색인가요?)\",\n",
      "        \"Is it typically eaten raw? (보통 생으로 먹나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": false,\n",
      "    \"is_end\": false,\n",
      "    \"message\": \"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요. Is it a type of fruit? (과일의 종류인가요?)\"\n",
      "}\n",
      "\n",
      "LLM1: {\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 0,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it red or green? (빨간색인가요, 아니면 초록색인가요?)\",\n",
      "        \"Is it typically eaten raw? (보통 생으로 먹나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": false,\n",
      "    \"is_end\": false,\n",
      "    \"message\": \"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요. Is it a type of fruit? (과일의 종류인가요?)\"\n",
      "}\n",
      "LLM2: 저는 과일의 종류라고 생각해요! \"사과\"는 과일이죠. 그래서 제 대답은 \"사과\"입니다. 맞나요?\n",
      "\n",
      "LLM2: 저는 과일의 종류라고 생각해요! \"사과\"는 과일이죠. 그래서 제 대답은 \"사과\"입니다. 맞나요?\n",
      "LLM1: ```json\n",
      "{\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 0,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it a type of fruit? (과일의 종류인가요?)\",\n",
      "        \"Is it typically red or green? (주로 빨간색 또는 초록색인가요?)\",\n",
      "        \"Can you eat it raw? (생으로 먹을 수 있나요?)\",\n",
      "        \"Does it have a core with seeds? (씨가 있는 중심부가 있나요?)\",\n",
      "        \"Is it commonly used in pies? (파이에 자주 사용되나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": false,\n",
      "    \"is_end\": false,\n",
      "    \"message\": \"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요. Is it a type of fruit? (과일의 종류인가요?)\"\n",
      "}\n",
      "```\n",
      "\n",
      "LLM1: ```json\n",
      "{\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 0,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it a type of fruit? (과일의 종류인가요?)\",\n",
      "        \"Is it typically red or green? (주로 빨간색 또는 초록색인가요?)\",\n",
      "        \"Can you eat it raw? (생으로 먹을 수 있나요?)\",\n",
      "        \"Does it have a core with seeds? (씨가 있는 중심부가 있나요?)\",\n",
      "        \"Is it commonly used in pies? (파이에 자주 사용되나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": false,\n",
      "    \"is_end\": false,\n",
      "    \"message\": \"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요. Is it a type of fruit? (과일의 종류인가요?)\"\n",
      "}\n",
      "```\n",
      "LLM2: 좋아요! 제가 생각하고 있는 것은 \"사과\"입니다. 그럼 질문에 답해볼게요. \n",
      "\n",
      "1. 네, 과일의 종류입니다.\n",
      "2. 주로 빨간색 또는 초록색입니다.\n",
      "3. 네, 생으로 먹을 수 있습니다.\n",
      "4. 네, 씨가 있는 중심부가 있습니다.\n",
      "5. 네, 파이에 자주 사용됩니다.\n",
      "\n",
      "제 대답이 맞나요?\n",
      "\n",
      "LLM2: 좋아요! 제가 생각하고 있는 것은 \"사과\"입니다. 그럼 질문에 답해볼게요. \n",
      "\n",
      "1. 네, 과일의 종류입니다.\n",
      "2. 주로 빨간색 또는 초록색입니다.\n",
      "3. 네, 생으로 먹을 수 있습니다.\n",
      "4. 네, 씨가 있는 중심부가 있습니다.\n",
      "5. 네, 파이에 자주 사용됩니다.\n",
      "\n",
      "제 대답이 맞나요?\n",
      "LLM1: {\n",
      "    \"total_round\": 1,\n",
      "    \"answer_count\": 1,\n",
      "    \"current_answer\": \"사과\",\n",
      "    \"hint\": [\n",
      "        \"Is it a type of fruit? (이것은 과일의 종류인가요?)\",\n",
      "        \"Is it mostly red or green? (주로 빨간색 또는 초록색인가요?)\",\n",
      "        \"Can it be eaten raw? (생으로 먹을 수 있나요?)\",\n",
      "        \"Does it have a core with seeds? (씨가 있는 중심부가 있나요?)\",\n",
      "        \"Is it often used in pies? (파이에 자주 사용되나요?)\"\n",
      "    ],\n",
      "    \"check_answer\": true,\n",
      "    \"is_end\": true,\n",
      "    \"message\": \"역시 제가 맞죠!🎉💪\"\n",
      "}\n",
      "\n",
      "Correcting total_round from 1 to 2\n",
      "\n",
      "Conversation log saved to 'conversation_log.txt'.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize conversation log\n",
    "conversation_log = []\n",
    "\n",
    "# Set persona for each LLM in system messages\n",
    "system_message1 = SystemMessage(content=\"\"\"# Purpose\n",
    "- 당신은 사용자와 게임을 진행하는 영어 선생님 역할을 수행합니다.\n",
    "- 다음 규칙을 따르는 영어게임을 진행해주세요.\n",
    "\n",
    "<Game>\n",
    "## Game Overview\n",
    "- 당신은 사용자가 선택한 단어를 맞추어야 합니다.\n",
    "- 당신은 사용자에게 해당 단어에 대한 질문을 할 수 있습니다.\n",
    "    - 질문은 영문이어야 하며 한국어 번역이 함께 있어야 합니다.  \n",
    "\n",
    "## Game Process\n",
    "1. 게임 시작 시 total_round는 1로 시작합니다.\n",
    "    - 게임 시작시 다음 문구와 함께 게임을 시작합니다:\n",
    "<start_message>\n",
    "\"저는 사람의 마음을 읽을 수 있어요. 여러분이 무슨 생각을 하고 있는지 맞춰볼게요.\" + hint의 질문 중 가장 current_answer와 관련된 질문\n",
    "</start_message>\n",
    "\n",
    "2. 매 라운드마다 다음을 진행합니다:\n",
    "    - 당신이 생각하는 정답을 current_answer 의 value 값으로 지정합니다.\n",
    "    - current_answer를 정답이라 가정할 때, 정답을 확정할 수 있을 질문 다섯 개를 hint에 지정합니다.\n",
    "    - hint의 질문 중 가장 current_answer와 관련된 질문 하나를 message에 출력합니다.\n",
    "        - 같은 질문을 반복하지 않습니다.\n",
    "\n",
    "3. 사용자 응답에 따른 처리:\n",
    "    - Yes인 경우:\n",
    "        - check_answer를 True로 설정\n",
    "        - answer_count를 1 증가\n",
    "        - current_answer를 답으로 확신하는 경우, message에 current_answer를 출력합니다.\n",
    "            - 그렇지 않으면 다음 라운드로 진행\n",
    "                - current_answer를 유지합니다.\n",
    "    - No인 경우:\n",
    "        - check_answer를 False로 설정\n",
    "        - answer_count를 1 증가\n",
    "        - 다음 라운드로 진행\n",
    "            - 만약 current_answer에 의문이 든다면 current_answer를 바꿉니다.\n",
    "                - 사용자의 대답이 current_answer와 맞지 않는다면 current_answer를 조건에 부합하는 단어로 반드시 바꿉니다.\n",
    "                    - e.g. Is it a type of fruit? -> No -> No fruit for answer should be selected \n",
    "            - 만약 total_round == 5이면 message에 current_answer를 출력합니다.\n",
    "\n",
    "## Ending\n",
    "- 성공 조건\n",
    "    - answer_count가 5 이하 이고 message에 출력된 current_answer에 대해 사용자가 Yes라 응답한 경우, is_end를 True로 설정합니다.\n",
    "    - message에는 \"역시 제가 맞죠!🎉💪\" 와 같은 성공 및 종료 안내 메시지를 출력합니다.\n",
    "\n",
    "- 실패 조건\n",
    "    - total_round가 5이고 message에 출력된 current_answer에 대해 No가 돌아왔다면, is_end를 True로 설정합니다.\n",
    "    - message에는 \"아깝군요!🌟\" + {{틀린 이유}} 와 같은 실패 메시지를 출력합니다.\n",
    "\n",
    "</Game>\n",
    "\n",
    "# Input format\n",
    "- 사용자의 입력은 다음과 같은 형식을 따릅니다:\n",
    "<input> \n",
    "string\n",
    "</input>\n",
    "\n",
    "# Output format\n",
    "- 출력시 json 형식만 출력하세요.\n",
    "- 출력값은 다음과 같은 json 형식을 따릅니다:\n",
    "{\n",
    "    \"total_round\": int(1~5), \"총 진행할 라운드를 출력합니다.\",\n",
    "    \"answer_count\": int(0~3), \"현재까지 맞춘 문제의 개수를 출력합니다.\",\n",
    "    \"current_answer\": str, \"현재 진행중인 문제의 정답에 해당하는 한글 단어를 출력합니다.\",\n",
    "    \"hint\": list[str], \"현재 진행중인 문제에 대한 힌트를 2개 출력합니다.\",\n",
    "    \"check_answer\": bool, \"사용자가 정답을 맞추었는지 확인하는 여부를 boolean 형식으로 출력합니다.\",\n",
    "    \"is_end\": bool, \"게임의 종료 여부를 boolean 형식으로 출력합니다.\",\n",
    "    \"message\": str, \"사용자에게 제공될 상황과 메시지를 출력합니다.\"\n",
    "}\"\"\")\n",
    "system_message2 = SystemMessage(content=\"\"\"게임의 참여자가 되어.\"\"\")\n",
    "\n",
    "# Set an initial prompt for the first LLM\n",
    "initial_prompt = \"안녕\"\n",
    "\n",
    "# Initialize game state variables\n",
    "total_round = 1\n",
    "answer_count = 0\n",
    "max_rounds = 10  # Maximum number of rounds allowed\n",
    "\n",
    "# Start the conversation\n",
    "print(\"Starting conversation between LLM1 and LLM2 with personas set in system messages...\\n\")\n",
    "response1 = llm1([system_message1, HumanMessage(content=initial_prompt)])\n",
    "conversation_log.append({\"LLM1\": initial_prompt})\n",
    "conversation_log.append({\"LLM2\": response1.content})\n",
    "print(f\"LLM1: {initial_prompt}\")\n",
    "print(f\"LLM2: {response1.content}\\n\")\n",
    "\n",
    "# Iterate conversation for a few rounds\n",
    "for round_num in range(max_rounds):\n",
    "    # Process LLM1's response and validate total_round\n",
    "    prompt1 = response1.content\n",
    "    response2 = llm2([system_message2, HumanMessage(content=prompt1)])\n",
    "    conversation_log.append({\"LLM1\": prompt1})\n",
    "    conversation_log.append({\"LLM2\": response2.content})\n",
    "    print(f\"LLM1: {prompt1}\")\n",
    "    print(f\"LLM2: {response2.content}\\n\")\n",
    "\n",
    "    prompt2 = response2.content\n",
    "    response1 = llm1([system_message1, HumanMessage(content=prompt2)])\n",
    "    conversation_log.append({\"LLM2\": prompt2})\n",
    "    conversation_log.append({\"LLM1\": response1.content})\n",
    "    print(f\"LLM2: {prompt2}\")\n",
    "    print(f\"LLM1: {response1.content}\\n\")\n",
    "\n",
    "    # Parse LLM1 output (expected in JSON format) and validate game state\n",
    "    try:\n",
    "      if response1.content.startswith(\"{\"):\n",
    "        llm1_output = json.loads(response1.content)  # Parse JSON safely\n",
    "        if llm1_output.get(\"total_round\", total_round) != total_round:\n",
    "            print(f\"Correcting total_round from {llm1_output['total_round']} to {total_round}\")\n",
    "            llm1_output[\"total_round\"] = total_round\n",
    "        if llm1_output.get(\"check_answer\", False):\n",
    "            answer_count += 1\n",
    "        total_round += 1\n",
    "        if total_round > max_rounds or llm1_output.get(\"is_end\", False):\n",
    "            break\n",
    "      else:\n",
    "        llm1_output = {\"message\": response1.content}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing LLM1 output: {e}\")\n",
    "        break\n",
    "\n",
    "# Save conversation log to a file\n",
    "with open(\"conversation_log.txt\", \"w\", encoding=\"utf-8\") as log_file:\n",
    "    for entry in conversation_log:\n",
    "        for key, value in entry.items():\n",
    "            log_file.write(f\"{key}: {value}\\n\")\n",
    "    print(\"\\nConversation log saved to 'conversation_log.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
